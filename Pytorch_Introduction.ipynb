{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l7FN9ytvt_Mo"
      ],
      "authorship_tag": "ABX9TyNKa8RCPA9sNJDBFbbsUXkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudevico/Gachon-AISTUDY/blob/main/Pytorch_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. tensor data type\n",
        "* 기본적으로 Machine Learning에서는 연산 속도를 위해서 array data type을 사용한다.\n",
        "* 이때 Numpy에서는 CPU를 사용하여 array를 연산하기 때문에 DL 수준의 dataset에 대해서 연산하기에는 시간이 많이 소요된다.\n",
        "* Tensor는 GPU 연산을 지원하는 array이다. pytorch에서는 이를 사용한다.  \n",
        "\n",
        "> 왜 Numpy ndarray가 아닌, Pytorch tensor를 사용하는지에 대한 자세한 이유들은 이후 실습 과정을 통해서 자연스럽게 알 수 있을 것으로 판단된다."
      ],
      "metadata": {
        "id": "ANng_D3WI0dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 0. Convert list(python) or ndarray(numpy) to tensor(pytorch)\n",
        "\n",
        "cf. [torch.tensor — PyTorch 2.4 documentation](https://pytorch.org/docs/stable/generated/torch.tensor.html)\n",
        "\n",
        "```\n",
        "torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)\n",
        "```\n",
        "\n",
        "**Parameters**  \n",
        "* __data__ - Can be a list, tuple, Numpy `ndarray`, scalar, ...  \n",
        "\n",
        "**Keyword Arguements**  \n",
        "* __dtype__(`torch.dtype`, optinal) - the desired data type of returned tensor. default라면, 원본 데이터 타입\n",
        "* __device__(`torch.device`, optional) - the device of the constructed tensor. defalut라면, 'cpu' 사용, 'cuda'로 설정 시 gpu 사용 가능.\n",
        "* __requires_grad__(_bool_, optional)\n",
        "* **pin_memory**(*bool*, optional)"
      ],
      "metadata": {
        "id": "pIpjnr_OKcXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# case1-1. python list_int\n",
        "data = [[1, 2, 3], [4, 5, 6]]\n",
        "\n",
        "x_default = torch.tensor(data)\n",
        "x_int = torch.tensor(data, dtype=int)\n",
        "x_int64 = torch.tensor(data, dtype=torch.int64) # default int\n",
        "x_int32 = torch.tensor(data, dtype=torch.int32)\n",
        "\n",
        "# pytorch에서는 int ßßdata type의 default를 64bit로 한다\n",
        "print(x_default.dtype)\n",
        "print(x_int.dtype)\n",
        "print(x_int64.dtype)\n",
        "print(x_int32.dtype)\n",
        "\n",
        "# pytorch에서는 tensor의 data type이 default가 아닌 경우에만 return한다\n",
        "print(x_default)    # default -> no return\n",
        "print(x_int)        # default -> no return\n",
        "print(x_int64)      # default -> no return\n",
        "print(x_int32)      # not default -> return"
      ],
      "metadata": {
        "id": "G5m9i8faGUA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fa9aca-e74b-40bd-b767-3befc7ce10af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n",
            "torch.int64\n",
            "torch.int64\n",
            "torch.int32\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# case1-2. python list_float\n",
        "data = [[1, 2, 3], [4, 5, 6]]\n",
        "\n",
        "x_float = torch.tensor(data, dtype=float)\n",
        "x_float32 = torch.tensor(data, dtype=torch.float32) # default float\n",
        "x_float64 = torch.tensor(data, dtype=torch.float64)\n",
        "x_double = torch.tensor(data, dtype=torch.double)\n",
        "\n",
        "# pytorch에서는 int data type의 default를 32bit로 한다\n",
        "print(x_float.dtype)\n",
        "print(x_float32.dtype) # default float\n",
        "print(x_float64.dtype)\n",
        "print(x_double.dtype)\n",
        "\n",
        "# pytorch에서는 tensor의 data type이 default가 아닌 경우에만 return한다\n",
        "print(x_float)      # not default -> return\n",
        "print(x_float32)    # default -> no return\n",
        "print(x_float64)    # not default -> return\n",
        "print(x_double)     # not default -> return"
      ],
      "metadata": {
        "id": "fdI5yDY6SRf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3f5a14-9281-4ace-b569-34663a61eba7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.float32\n",
            "torch.float64\n",
            "torch.float64\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64)\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64)\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# case2. numpy to tensor\n",
        "# 결론적으로는 list to tensor나 ndarray to tensor나 동일하다.\n",
        "import numpy as np\n",
        "np_data = np.array(data)\n",
        "print(type(np_data))\n",
        "print(np_data)"
      ],
      "metadata": {
        "id": "PBreb74oTSDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e625757-dc8b-4ca9-c455-8565defc294f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# case2-1. numpy ndarray_int\n",
        "x_default = torch.tensor(np_data)\n",
        "x_int = torch.tensor(np_data, dtype=int)\n",
        "x_int64 = torch.tensor(np_data, dtype=torch.int64) # default int\n",
        "x_int32 = torch.tensor(np_data, dtype=torch.int32)\n",
        "\n",
        "# pytorch에서는 int data type의 default를 64bit로 한다\n",
        "print(x_default.dtype)\n",
        "print(x_int.dtype)\n",
        "print(x_int64.dtype)\n",
        "print(x_int32.dtype)\n",
        "\n",
        "# pytorch에서는 tensor의 data type이 default가 아닌 경우에만 return한다\n",
        "print(x_default)    # default -> no return\n",
        "print(x_int)        # default -> no return\n",
        "print(x_int64)      # default -> no return\n",
        "print(x_int32)      # not default -> return"
      ],
      "metadata": {
        "id": "xVc_mJQ5T75Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af56b635-b195-4465-ccb9-2aed0da28dc0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n",
            "torch.int64\n",
            "torch.int64\n",
            "torch.int32\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# case2-2. numpy ndarray_float\n",
        "data = [[1, 2, 3], [4, 5, 6]]\n",
        "\n",
        "x_float = torch.tensor(np_data, dtype=float)\n",
        "x_float32 = torch.tensor(np_data, dtype=torch.float32) # default float\n",
        "x_float64 = torch.tensor(np_data, dtype=torch.float64)\n",
        "x_double = torch.tensor(np_data, dtype=torch.double)\n",
        "\n",
        "# pytorch에서는 int data type의 default를 32bit로 한다\n",
        "print(x_float.dtype)\n",
        "print(x_float32.dtype) # default float\n",
        "print(x_float64.dtype)\n",
        "print(x_double.dtype)\n",
        "\n",
        "# pytorch에서는 tensor의 data type이 default가 아닌 경우에만 return한다\n",
        "print(x_float)      # not default -> return\n",
        "print(x_float32)    # default -> no return\n",
        "print(x_float64)    # not default -> return\n",
        "print(x_double)     # not default -> return"
      ],
      "metadata": {
        "id": "y_ipKbkVUK1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab525633-53fe-48f8-85ae-e8053c846cb6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.float32\n",
            "torch.float64\n",
            "torch.float64\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64)\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64)\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 1. How to use GPU(CUDA)"
      ],
      "metadata": {
        "id": "mpTv64UyUl-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch를 실행중인 device에서 cuda(gpu)를 사용할 수 있는지 확인\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(cuda_available)"
      ],
      "metadata": {
        "id": "_uiGPsRvIpfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50caba7-bfb3-4fd2-dda0-33a58ba8183a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.tensor() 등으로 tensor를 생성할 때는 default로 'cpu'를 사용\n",
        "print(x_default.device)"
      ],
      "metadata": {
        "id": "ogrRdUj0QKq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cd5994-afd1-4133-a68a-5adf44752578"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU에 텐서 생성 (default)\n",
        "tensor_cpu = torch.tensor([1.0, 2.0, 3.0]) # same as the below\n",
        "tensor_cpu = torch.tensor([1.0, 2.0, 3.0], device='cpu') # same as the above\n",
        "print(tensor_cpu.device) # cpu\n",
        "\n",
        "# GPU에 텐서 생성 (CUDA를 사용할 수 있는 경우)\n",
        "if torch.cuda.is_available():\n",
        "    tensor_gpu = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
        "    print(tensor_gpu.device)  # cuda:0\n",
        "else:\n",
        "    print('Error: CUDA not available')"
      ],
      "metadata": {
        "id": "OUDAKK0sVV3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e00c3d-0598-4ddc-90f3-5a46b33f8b1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 텐서를 다른 장치로 이동\n",
        "tensor_cpu_to_gpu = tensor_cpu.to('cuda')  # CPU에서 GPU로 이동\n",
        "print(tensor_cpu_to_gpu.device)  # cuda:0\n",
        "\n",
        "tensor_gpu_to_cpu = tensor_gpu.to('cpu')  # GPU에서 CPU로 이동\n",
        "print(tensor_gpu_to_cpu.device)  # cpu"
      ],
      "metadata": {
        "id": "LDyidBPUtRe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f50a578-c152-49e3-9415-2b9197c9d003"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 2. How to create the tensor"
      ],
      "metadata": {
        "id": "l7FN9ytvt_Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. non-random values\n",
        "\n",
        "# 1-dim tensor\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x)\n",
        "print()\n",
        "\n",
        "# 2-dim tensor\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(y)\n",
        "print()\n",
        "\n",
        "# 3-dim tensor, every element is 0\n",
        "z = torch.zeros((2, 3, 4))\n",
        "print(z)\n",
        "print()\n",
        "\n",
        "# 4-dim tensor, every element is 1\n",
        "w = torch.ones((2, 2, 2, 2))\n",
        "print(w)\n",
        "print()"
      ],
      "metadata": {
        "id": "abESN7TfvWa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f687b09-a934-421c-b739-1e0d4adc6d94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "\n",
            "tensor([[[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. use the Distribution for random values\n",
        "\n",
        "# case1. Uniform Distribution\n",
        "# create 3 by 2 tensor, each element is randomed between [0, 1)\n",
        "x = torch.rand((3, 2))\n",
        "print(x)\n",
        "\n",
        "# case 2. Normal Distribution\n",
        "# create 2 by 3 tensor, each element is randomed,\n",
        "# following normal distribution, mean = 0 and deviation = 1\n",
        "y = torch.randn((2, 3))\n",
        "print(y)"
      ],
      "metadata": {
        "id": "jWoDyzKmxmVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055ed9dc-3d09-4a94-8e7d-5fadc518bcb8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1554, 0.1935],\n",
            "        [0.7643, 0.8239],\n",
            "        [0.3334, 0.1178]])\n",
            "tensor([[ 2.7997,  0.1898, -0.5784],\n",
            "        [-0.4324,  0.2793, -1.4159]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. randomly-uninitialized values(it is fast because non-intialize)\n",
        "\n",
        "x = torch.empty(2, 3) # default dtype is float32\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "\n",
        "y = torch.empty(2, 3, dtype=torch.int64) # you can use anoter dtype\n",
        "print(y)\n",
        "print(y.dtype)"
      ],
      "metadata": {
        "id": "tQCmICdR2j7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce5fb8d-2b2b-40ae-a138-523b16a8f66f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[8.2631e+20, 4.3690e-41, 8.2631e+20],\n",
            "        [4.3690e-41, 4.4842e-44, 0.0000e+00]])\n",
            "torch.float32\n",
            "tensor([[ 97185518881915, 133910137875808,              64],\n",
            "        [             80,     23729545995,               0]])\n",
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 3. Convet ndarray to tensor by `.from_numpy()` and `.tensor()`\n",
        "**1. 0.**에서는 `.tensor()`를 사용해서 numpy array를 tensor로 변환했다.  \n",
        "`.from_numpy()`로도 변환이 가능한데, 둘의 차이를 알아보자.\n",
        "```\n",
        "x_np = np.array([1, 2, 3])\n",
        "x_tensor = torch.tensor(x_np)\n",
        "print(type(x_np), type(x_tensor))\n",
        "\n",
        "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
        "```\n",
        "\n",
        "```\n",
        "y_np = np.array([1, 2, 3])\n",
        "y_tensor = torch.from_numpy(y_np)\n",
        "print(type(x_np), type(y_tensor))\n",
        "\n",
        "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
        "```"
      ],
      "metadata": {
        "id": "pibQHdZj4_sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `.tensor()`는 call by value 개념이다\n",
        "# 즉 기존의 ndarray를 copy해서 memory에 별도로 생성한다\n",
        "# 따라서 tensor와 ndarray는 서로 독립적이다\n",
        "x_np = np.array([1, 2, 3])\n",
        "x_tensor = torch.tensor(x_np)\n",
        "print('ndarray:', x_np[0], 'tensor:', x_tensor[0])\n",
        "\n",
        "# tesnor의 element를 바꿔도 ndarray에는 영향을 주지 않는다\n",
        "x_tensor[0] = -1\n",
        "print('ndarray:', x_np[0], 'tensor:', x_tensor[0])"
      ],
      "metadata": {
        "id": "ysOhzuft7VL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d166acf7-18c2-4749-d8e1-0acddc2de777"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ndarray: 1 tensor: tensor(1)\n",
            "ndarray: 1 tensor: tensor(-1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# `.from_numpy()`는 call by reference 개념이다\n",
        "# 이미 ndarray가 할당된 memory adress가 존재할 때,\n",
        "# `.from_numpy()`로 생성된 tensor 객체는 ndarray와 동일한 address를 referencing한다\n",
        "y_np = np.array([1, 2, 3])\n",
        "y_tensor = torch.from_numpy(y_np)\n",
        "print('ndarray:', y_np[0], 'tensor:', y_tensor[0])\n",
        "\n",
        "# tesnor의 element를 바꾸면 ndarray에도 영향을 준다\n",
        "y_tensor[0] = -1\n",
        "print('ndarray:', y_np[0], 'tensor:', y_tensor[0])\n",
        "\n",
        "# 반대의 경우에도 마찬가지\n",
        "y_np[1] = -2\n",
        "print('ndarray:', y_np[1], 'tensor:', y_tensor[1])"
      ],
      "metadata": {
        "id": "iCYv34sf8XbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf01f73-5f8c-41c5-91f6-93b52739750f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ndarray: 1 tensor: tensor(1)\n",
            "ndarray: -1 tensor: tensor(-1)\n",
            "ndarray: -2 tensor: tensor(-2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 3.(검증) memory address 확인을 통한 검증"
      ],
      "metadata": {
        "id": "L1Be-F_h-xqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 처음에 내가 아는 선에서 시도한 검증은 다음과 같이 `id()`를 사용하는 것이었다\n",
        "print(id(x_np), id(x_tensor)) # predict: different  | actual: different\n",
        "print(id(y_np), id(y_tensor)) # predict: different  | actual: different"
      ],
      "metadata": {
        "id": "lSj3agKg9TON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab82dd5b-08bc-4cce-c582-40673704a13e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133906103567824 133906103465632\n",
            "133906103568496 133906103582944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> numpy와 tensor 모두 memory address에 value 자체를 보관하고, **그 address를 참조하는 객체**를 생성하는 것이다.  \n",
        "\n",
        "> 즉 value 자체의 address와 참조하는 객체의 address 두 가지가 존재하는 상태이다.  \n",
        "이때 `id()`는 후자를 리턴한다.  \n",
        "\n",
        "> (다음 cell) 다음과 같이 **value 자체의 address**를 확인할 수 있다."
      ],
      "metadata": {
        "id": "ynWrjz6h_jpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict: different    | actual: different\n",
        "print(x_np.__array_interface__['data'][0])\n",
        "print(x_tensor.data_ptr())\n",
        "print()\n",
        "# predict: same         | actual: same\n",
        "print(y_np.__array_interface__['data'][0])\n",
        "print(y_tensor.data_ptr())"
      ],
      "metadata": {
        "id": "m8e-XY2I-dKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7e018c-2b92-4639-9d85-88331cc86a1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97196112915664\n",
            "97196202511040\n",
            "\n",
            "97196112915696\n",
            "97196112915696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 4. Convert tensor to ndarray\n",
        "tensor 객체를 Numpy ndarray로 변환하는 method는 `tensor.numpy()` method 뿐이다.  \n",
        "\n",
        "또한 `tensor.numpy()`는 copy하지 않고, 무조건 share한다."
      ],
      "metadata": {
        "id": "rZACgFWBBhWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_npnp = x_tensor.numpy()\n",
        "print(type(x_npnp))\n",
        "\n",
        "y_npnp = y_tensor.numpy()\n",
        "print(type(y_npnp))\n",
        "\n",
        "print(x_np.__array_interface__['data'][0])\n",
        "print(x_tensor.data_ptr()) # x_np를 copy한 x_tensor\n",
        "print(x_npnp.__array_interface__['data'][0]) # x_tensor를 share한 x_npnp\n",
        "print()\n",
        "\n",
        "print(y_np.__array_interface__['data'][0])\n",
        "print(y_tensor.data_ptr()) # y_np를 share한 y_tensor\n",
        "print(y_npnp.__array_interface__['data'][0]) # y_tensor를 share한 y_npnp"
      ],
      "metadata": {
        "id": "Ib9mK0z3EAKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ffd080-4a12-4fef-eefd-aed535f06f3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "97196112915664\n",
            "97196202511040\n",
            "97196202511040\n",
            "\n",
            "97196112915696\n",
            "97196112915696\n",
            "97196112915696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "또한 Numpy는 기본적으로 gpu 사용을 지원하지 않기 때문에, Numpy로 변환하려는 tensor 객체가 gpu에 생성되어 있다면 이를 cpu상의 tensor로 먼저 변환한 후에 Numpy로 변환해야 한다."
      ],
      "metadata": {
        "id": "Jrg_A1Kp2NV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_tensor = torch.tensor([1, 2, 3], device='cuda')\n",
        "temp_tensor.numpy() # error occured!"
      ],
      "metadata": {
        "id": "a7QeHKx92dWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "a707b6c3-9817-4c25-efe1-9fbe27a38f3e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-aa5b073a5a97>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# error occured!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_tensor.to('cpu').numpy() # no error!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN5zf6FE3aW_",
        "outputId": "f843c60d-bb53-4390-e917-b7274ebe1f7d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. tensor attributes\n",
        "Pytorch의 **tensor object**는 많은 attributes를 갖지만, 그 중에서 자주 쓸 만한 것들에 대해서만 다루겠다.  \n",
        "1. __dtype__\n",
        "    * data type\n",
        "2. __device__\n",
        "    * tensor object가 위치한 device(cpu, gpu)\n",
        "3. __size()__, __shape__\n",
        "    * tensor object의 shape\n",
        "4. __ndim__\n",
        "    * tensor object의 dimension\n",
        "5. __numel()__\n",
        "    * tensor object의 전체 elements 수(number of elements)"
      ],
      "metadata": {
        "id": "y4Max_dxE53G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'dtype', 'device'는 위에서 이미 다뤘으므로 생략함\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "y = x.view(2, 2) # same as reshape in Numpy\n",
        "\n",
        "print(y) # [[1, 2], [3, 4]]\n",
        "print(x.size(), y.size()) # [4] [2, 2] # same as shape in Numpy\n",
        "print(x.shape, y.shape) # ㄴ 그냥 numpy에서 하던 대로 shape 쓰는게 나을 듯 둘이 똑같음\n",
        "print(x.ndim, y.ndim) # 1 2\n",
        "print(x.numel(), y.numel()) # 4 4 # number of elements. same as size in Numpy"
      ],
      "metadata": {
        "id": "ITA9FadGGdxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb0ee2a-a20d-4218-bdc8-c853274dc368"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "torch.Size([4]) torch.Size([2, 2])\n",
            "torch.Size([4]) torch.Size([2, 2])\n",
            "1 2\n",
            "4 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. tensor calculation\n",
        "Numpy와 마찬가지로 **Broadcasting**이 적용된다."
      ],
      "metadata": {
        "id": "C1iKeN1tQqSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# addition in tensor\n",
        "a = torch.tensor([[1, 2,], [3, 4]])\n",
        "b = torch.tensor([[5, 6], [7, 8]])\n",
        "var_add = a + b\n",
        "print(var_add)"
      ],
      "metadata": {
        "id": "-SJbIwaCQ3-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f41e8df-04f6-478d-ab49-21e1951cfb16"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 6,  8],\n",
            "        [10, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# subtraction in tensor\n",
        "var_sub = a - b\n",
        "print(var_sub)"
      ],
      "metadata": {
        "id": "A0MLiKCNROrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c147c6d1-0c93-4733-832f-ed74b27883e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4, -4],\n",
            "        [-4, -4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply in tensor\n",
        "var_mul = a * b\n",
        "print(var_mul)"
      ],
      "metadata": {
        "id": "11E85enDRxva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fc2b99-ab24-43cf-b443-26fe98e22ec0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5, 12],\n",
            "        [21, 32]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide in tensor\n",
        "var_div = a / b\n",
        "print(var_div)"
      ],
      "metadata": {
        "id": "m0hBCwF_vJe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29275d7-5584-45a9-b452-dc96910d8f05"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2000, 0.3333],\n",
            "        [0.4286, 0.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In mathematics, particularly in linear algebra, matrix multiplication is a binary operation that produces a matrix from two matrices. For matrix multiplication, **the number of columns in the first matrix** must be equal to **the number of rows in the second matrix**. The resulting matrix, known as the matrix product, has **the number of rows of the first** and **the number of columns of the second** matrix. The product of matrices A and B is denoted as AB.  \n",
        "* $C_{11} = A.row_1 * B.col_1$\n",
        "* $C_{12} = A.row_1 * B.col_2$\n",
        "* $C_{21} = A.row_2 * B.col_1$\n",
        "* $C_{22} = A.row_2 * B.col_2$"
      ],
      "metadata": {
        "id": "2pjvDp8cxVUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication\n",
        "var_matrix_mul = torch.mm(a, b)\n",
        "print(var_matrix_mul)"
      ],
      "metadata": {
        "id": "sH7kdxk1vjto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c429e216-97eb-43ad-812c-51c8f53ee6e9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[19, 22],\n",
            "        [43, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In mathematics, the dot product or scalar product is an algebraic operation that **takes two equal-length sequences of numbers** (usually coordinate vectors), and **returns a single number**. It is often called the inner product."
      ],
      "metadata": {
        "id": "R3nyTxlczpjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dot product in tensor\n",
        "var_dot_prod = torch.dot(a[0], a[1])\n",
        "print(var_dot_prod)"
      ],
      "metadata": {
        "id": "SvCd0Kc6vjzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371f14ca-7a59-4466-ca2f-c7fc13ed9dd8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max value\n",
        "#                  col1    col2\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "b = torch.tensor([[4, 3], [2, 1]])\n",
        "var_max_a = torch.max(a)\n",
        "var_max_b = torch.max(b)\n",
        "var_max_ab = torch.max(a, b)\n",
        "print(var_max_a, var_max_b) # 4 4\n",
        "# column끼리 비교해서 max를 리턴함\n",
        "# max_col1, max_col2\n",
        "print(var_max_ab) # [4, 3], [3, 4]"
      ],
      "metadata": {
        "id": "quuNPJ1I0KfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cb37a3-685b-4b70-d9cd-e646d28ec2e1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4) tensor(4)\n",
            "tensor([[4, 3],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dimensionality reduction; 차원 축소\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "\n",
        "# dim=0, 하나의 matrix의 각 row를 개별 matrix로 취급하여 dot product를 수행하는 개념\n",
        "var_reduce_dim0 = torch.sum(a, dim=0)\n",
        "# dim=1\n",
        "var_reduce_dim1 = torch.sum(a, dim=1)\n",
        "\n",
        "print(var_reduce_dim0)\n",
        "print(var_reduce_dim1)"
      ],
      "metadata": {
        "id": "h2xVK-e-1d_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cba78a-389f-4c3f-d5b8-74087e9c8c2c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 6])\n",
            "tensor([3, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transpose\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "# parameter is input tensor, first dim and second dim to transposed\n",
        "var_trans = torch.transpose(a, 0, 1)\n",
        "print(var_trans)"
      ],
      "metadata": {
        "id": "Kk_3fp5m2mVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b358c7e0-2f2e-49b6-8c0c-ecab16203034"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 3],\n",
            "        [2, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing\n",
        "#                   0       1       2\n",
        "#                  0  1    0  1    0  1\n",
        "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "var_index = a[1, 0]\n",
        "print(var_index)"
      ],
      "metadata": {
        "id": "Y6G94DUj3m33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1678abd9-5863-414a-a612-3467758529bd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# slicing\n",
        "var_slice_1 = a[:, :1] # for the all rows, col1 전까지\n",
        "             # '까지'니까 selected element가 1개라는 것이 보장되지 않음\n",
        "             # 따라서 2dim으로 리턴\n",
        "var_slice_2 = a[:, 1] # for the all rows, col1만\n",
        "            # '만'이니까 selected element가 1개라는 것이 보장됨\n",
        "            # 따라서 1dim으로 리턴\n",
        "print(var_slice_1)\n",
        "print(var_slice_2)"
      ],
      "metadata": {
        "id": "hoKXSINZ4AMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57288d70-da01-46ca-d4c5-63bbc953e2f2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1],\n",
            "        [3],\n",
            "        [5]])\n",
            "tensor([2, 4, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# boolean indexing\n",
        "a = torch.tensor([1, 2, 3, 4, 5])\n",
        "# statement 만족하면 True, 아니면 False가 되고 True인 index의 elements만 리턴\n",
        "var_bool = a[a > 3]\n",
        "print(var_bool)"
      ],
      "metadata": {
        "id": "Vc1QK6dE5WUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b90b27-0c8c-47fb-f7d1-9442b0e50245"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. tensor floating point\n",
        "torch에서는 floating point의 bit에 따라서 precision을 계산하는 것은 torch가 알아서 처리하고, 유저에게 출력을 리턴할 때는 precision과 관계없이 소수점 넷째 자리까지만 리턴한다."
      ],
      "metadata": {
        "id": "Bdoik0jX5o6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_f32 = torch.FloatTensor([1.0123456, 2, 3])\n",
        "tensor_f64 = torch.DoubleTensor([1.0123456789012345, 2, 3])\n",
        "print(tensor_f32.dtype)\n",
        "print(tensor_f32)\n",
        "print(tensor_f64.dtype)\n",
        "print(tensor_f64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSVOWE0Y58Fx",
        "outputId": "b823a2cb-f820-4630-9685-92d64affbd41"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "tensor([1.0123, 2.0000, 3.0000])\n",
            "torch.float64\n",
            "tensor([1.0123, 2.0000, 3.0000], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# default precision 변경\n",
        "torch.set_printoptions(precision=8)\n",
        "print(tensor_f32)\n",
        "print(tensor_f64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lfcO9-K65mI",
        "outputId": "c8bb0ab2-4059-4c0e-f446-5bd8c4396e89"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.01234555, 2.00000000, 3.00000000])\n",
            "tensor([1.01234568, 2.00000000, 3.00000000], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "[1] [머신러닝 파이토치 다루기 기초 - WikiDocs](https://wikidocs.net/book/9379)  \n",
        "[2] [PyTorch Tensor vs NumPy Array - GeeksforGeeks](https://www.geeksforgeeks.org/pytorch-tensor-vs-numpy-array/)  \n",
        "[3] [torch.tensor — PyTorch 2.4 documentation](https://pytorch.org/docs/stable/generated/torch.tensor.html)  "
      ],
      "metadata": {
        "id": "RydV314IFN30"
      }
    }
  ]
}