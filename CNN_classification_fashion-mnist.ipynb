{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f60253e",
   "metadata": {},
   "source": [
    "**Table of contents**  \n",
    "* Introduction  \n",
    "\n",
    "* DNN; Deep Neural Network  \n",
    "\n",
    "* CNN; Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de03fa-4e5c-4926-a631-a5f6cc742ab8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[PyTorch Tutorials](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#loading-a-dataset)  \n",
    "**Dataset: [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST)**  \n",
    "> The Fashion MNIST dataset is a large freely available database of fashion images that is commonly used for training and testing various machine learning systems.  \n",
    "Fashion-MNIST was intended to serve as a replacement for the original MNIST database for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits.  \n",
    "The dataset contains **70,000 28x28 grayscale images** of fashion products from **10 categories** from a dataset of Zalando article images, with 7,000 images per category.  \n",
    "The **training set consists of 60,000 images** and the **test set consists of 10,000 images**. The dataset is commonly included in standard machine learning libraries.\n",
    "\n",
    "**Framework**  \n",
    "* PyTorch: Version `2.1.0+cuda12.1`\n",
    "\n",
    "**Dependencies**  \n",
    "* Python: Version `3.9.19`\n",
    "* Numpy: Version `1.26.4`\n",
    "* Pandas: Version `2.2.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa5360-9c5e-49b2-aba1-89cd9b9eda58",
   "metadata": {},
   "source": [
    "# DNN; Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450a75a2-aa85-428d-bce6-1d630eb68951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms # for preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b461a91d-f147-43e7-ae2e-7f31c359135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch is running with [cuda:0]\n"
     ]
    }
   ],
   "source": [
    "# gpu가 사용 가능하면 torch가 gpu를 사용하도록 설정, 불가하면 cpu를 사용\n",
    "torch.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'torch is running with [{torch.device}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543bb972-a93c-4b58-abf8-2b23de94b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sample dataset 'FashionMNIST'\n",
    "# 해당 dataset은 애초에 train용, test용이 나눠져 있음\n",
    "\n",
    "# Convert PIL image format to tensor\n",
    "transform = transforms.ToTensor()\n",
    "# download & load the dataset\n",
    "# 초기 다운로드시 100%가 4개이면 모두 완료된 것임\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                  train=True,\n",
    "                                                  download=True,\n",
    "                                                  transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                 train=False,\n",
    "                                                 download=True,\n",
    "                                                 transform=transform)\n",
    "\n",
    "# cf. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#loading-a-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0ef96-8ed8-411b-b032-34f280731c1a",
   "metadata": {},
   "source": [
    "**cf. `ToTensor()`**  \n",
    "[ToTensor](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#totensor) converts a PIL image or Numpy `ndarray` into a `FloatTensor`. and scales the image's pixel intensity values in the range [0., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2cfc29f-b9c3-4d95-8709-31deb8d921cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./dataset\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# download & load 확인\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5308cf-7e8a-4724-9df6-be1f2f2dc75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the dataset\n",
    "\n",
    "# label(0~9)에 따른 제품명 mapping\n",
    "labels_map = {\n",
    "    0: 'T-Shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot'}\n",
    "\n",
    "figure = plt.figure(figsize=(9, 9))\n",
    "cols, rows = 4, 4\n",
    "\n",
    "# train_dataset의 sample을 무작위로 4*4개 만큼 골라서 visualize\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i) # 각 sample을 subplot으로 추가\n",
    "                                      # 1개의 큰 mainplot 안에 4*4개의 subplot\n",
    "    plt.title(labels_map[label]) # 각 subplot에 적용됨\n",
    "    plt.axis('off') # 축 범위 표시 off\n",
    "    # img.shape == (1, 28, 28) == (channel, height, width)임\n",
    "    # .squeeze()는 tensor의 dimension중 size가 1인 dimension을 제거함\n",
    "    # 따라서 img.squeeze() == (28, 28) == (height, width) 즉, 2-dim tensor\n",
    "    # plt.imshow()는 2-dim array를 visualize함\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97396bc4-5f5f-4a11-affa-c01d258760cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "60000\n",
      "28\n",
      "28\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0000, 0.0000, 0.0706, 0.4196, 0.4667, 0.4039,\n",
      "          0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.3882, 0.6078, 0.4431, 0.2392, 0.4627,\n",
      "          0.6784, 0.4588, 0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0039, 0.0000, 0.4314, 0.5333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.6549, 0.6235, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0235,\n",
      "          0.0000, 0.2824, 0.5765, 0.0000, 0.0000, 0.0196, 0.0000, 0.0078,\n",
      "          0.0000, 0.0000, 0.6824, 0.4627, 0.0000, 0.0196, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0196, 0.0000,\n",
      "          0.0000, 0.6824, 0.0157, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.8000, 0.1725, 0.0000, 0.0157, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.0000,\n",
      "          0.4902, 0.5020, 0.0000, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0078, 0.0000, 0.4196, 0.5961, 0.0000, 0.0235, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.7216, 0.0353, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0078, 0.0000, 0.0000, 0.7451, 0.0000, 0.0000, 0.0078,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0078, 0.0000, 0.2863,\n",
      "          0.6196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0157, 0.0000, 0.6157, 0.3098, 0.0000, 0.0118,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0078, 0.0196, 0.0039, 0.0000, 0.0000, 0.6157,\n",
      "          0.3843, 0.0000, 0.0078, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0196, 0.0000, 0.4745, 0.5686, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059,\n",
      "          0.3294, 0.0000, 0.0078, 0.0039, 0.0039, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0039, 0.0000, 0.3608, 0.8745, 0.0627, 0.0000,\n",
      "          0.0275, 0.0157, 0.0039, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.7176,\n",
      "          0.3176, 0.0000, 0.0000, 0.0000, 0.0078, 0.0078, 0.0039, 0.0078,\n",
      "          0.0039, 0.0000, 0.0039, 0.0000, 0.4706, 0.8863, 0.2235, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6784, 0.8471, 0.7569, 0.8353, 0.7176, 0.6431,\n",
      "          0.6549, 0.3843, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.4196, 0.5529, 0.4353, 0.3569,\n",
      "          0.3529, 0.4235, 0.1961, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7255, 0.8667, 0.8510, 0.8235, 0.7922, 0.8706,\n",
      "          0.7843, 0.8078, 0.7922, 0.8000, 0.4275, 0.1059, 0.0471, 0.0667,\n",
      "          0.2392, 0.5333, 0.7059, 0.8667, 0.7922, 0.8824, 0.8157, 0.8392,\n",
      "          0.8745, 0.9412, 0.6353, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7176, 0.8627, 0.8039, 0.7843, 0.7569, 0.7216,\n",
      "          0.7412, 0.7137, 0.6784, 0.7608, 0.8431, 0.8471, 0.8039, 0.8118,\n",
      "          0.8118, 0.7647, 0.7255, 0.7608, 0.7608, 0.7961, 0.8314, 0.7176,\n",
      "          0.7569, 0.8275, 0.6000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7451, 0.9137, 0.7961, 0.8078, 0.8392, 0.8471,\n",
      "          0.7647, 0.7176, 0.6902, 0.6235, 0.6863, 0.7412, 0.7922, 0.7647,\n",
      "          0.7294, 0.7294, 0.7137, 0.7294, 0.7647, 0.8196, 0.9216, 0.7961,\n",
      "          0.7922, 0.8431, 0.5333, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7059, 0.8941, 0.7882, 0.7922, 0.7373, 0.7176,\n",
      "          0.6980, 0.7529, 0.7294, 0.7098, 0.7098, 0.6941, 0.8000, 0.8510,\n",
      "          0.6863, 0.7020, 0.7216, 0.6902, 0.6510, 0.6627, 0.6863, 0.7255,\n",
      "          0.5961, 0.7961, 0.4196, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6549, 0.9216, 0.7843, 0.8118, 0.8118, 0.8196,\n",
      "          0.8118, 0.7961, 0.7765, 0.7412, 0.7137, 0.6980, 0.7294, 0.7412,\n",
      "          0.6980, 0.7529, 0.7725, 0.7647, 0.7529, 0.7255, 0.7216, 0.8039,\n",
      "          0.7333, 1.0000, 0.2392, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5961, 0.9804, 0.8157, 0.8392, 0.8196, 0.7922,\n",
      "          0.7843, 0.7922, 0.8039, 0.8000, 0.7843, 0.7529, 0.7765, 0.8000,\n",
      "          0.7647, 0.8039, 0.8078, 0.7765, 0.7804, 0.7961, 0.8392, 0.8118,\n",
      "          0.7020, 0.9765, 0.2157, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4941, 1.0000, 0.8275, 0.8431, 0.8235, 0.8078,\n",
      "          0.7961, 0.7961, 0.7961, 0.8078, 0.8000, 0.7804, 0.8078, 0.8118,\n",
      "          0.7843, 0.8078, 0.7882, 0.7804, 0.7922, 0.8157, 0.8431, 0.7647,\n",
      "          0.6824, 0.8275, 0.0588, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4196, 1.0000, 0.8314, 0.8275, 0.8235, 0.8275,\n",
      "          0.8157, 0.8078, 0.8078, 0.8118, 0.8157, 0.8157, 0.8510, 0.8392,\n",
      "          0.8039, 0.8078, 0.8000, 0.8157, 0.8275, 0.8275, 0.8627, 0.7725,\n",
      "          0.7137, 0.8824, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2471, 0.9098, 0.8275, 0.8471, 0.8314, 0.8392,\n",
      "          0.8314, 0.8314, 0.8392, 0.8314, 0.8275, 0.8314, 0.8745, 0.8588,\n",
      "          0.8235, 0.8392, 0.8353, 0.8314, 0.8275, 0.8196, 0.8510, 0.8157,\n",
      "          0.6588, 0.7451, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0549, 1.0000, 0.8549, 0.8667, 0.8431, 0.8549,\n",
      "          0.8549, 0.8510, 0.8431, 0.8353, 0.8275, 0.8431, 0.8941, 0.8627,\n",
      "          0.8471, 0.8706, 0.8510, 0.8510, 0.8471, 0.8549, 0.8314, 0.8353,\n",
      "          0.7412, 0.5608, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.9020, 0.8902, 0.8392, 0.8510, 0.8549,\n",
      "          0.8471, 0.8510, 0.8510, 0.8392, 0.8471, 0.8706, 0.8863, 0.8549,\n",
      "          0.8549, 0.8627, 0.8588, 0.8549, 0.8510, 0.8549, 0.8510, 0.8431,\n",
      "          0.8471, 0.4431, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.6941, 0.9137, 0.8471, 0.8745, 0.8706,\n",
      "          0.8353, 0.8353, 0.8431, 0.8314, 0.8627, 0.8863, 0.8667, 0.8627,\n",
      "          0.8667, 0.8549, 0.8706, 0.8627, 0.8627, 0.8667, 0.8510, 0.8314,\n",
      "          0.8588, 0.2039, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0392, 0.8745, 0.8706, 0.8627, 0.8275,\n",
      "          0.8353, 0.8431, 0.8627, 0.8706, 0.8863, 0.8863, 0.8667, 0.8745,\n",
      "          0.8706, 0.8706, 0.8667, 0.8549, 0.8627, 0.8588, 0.8627, 0.8745,\n",
      "          0.6824, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4941, 0.9451, 0.8157, 0.8235,\n",
      "          0.8392, 0.8392, 0.8471, 0.8471, 0.8627, 0.8627, 0.8392, 0.8353,\n",
      "          0.8314, 0.8314, 0.8235, 0.8431, 0.8510, 0.8549, 0.8431, 0.9255,\n",
      "          0.1647, 0.0000, 0.0039, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 0.9294, 0.9020,\n",
      "          0.9137, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000, 1.0000, 1.0000, 0.9059, 0.8980, 0.9373, 0.6314,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2471, 0.3725,\n",
      "          0.4235, 0.4118, 0.3922, 0.4039, 0.4078, 0.4118, 0.4000, 0.3922,\n",
      "          0.3843, 0.3804, 0.3765, 0.3529, 0.3137, 0.3255, 0.2353, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# cf. dataset의 sample 구성\n",
    "# dataset은 2-dimension으로 구성되어 있음\n",
    "# [0]은 샘플의 index, [1]은 해당 샘플의 label\n",
    "print(train_dataset[100][1]) # index 100 sample의 label == target\n",
    "print(len(train_dataset)) # 총 sample 수\n",
    "print(len(train_dataset[100][0][0])) # height pixel. 28\n",
    "print(len(train_dataset[100][0][0][27])) # width pixel. 28\n",
    "print(train_dataset[100][0]) # sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd29eae-d295-461f-94dc-786cc0a3750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 label: 8\n"
     ]
    }
   ],
   "source": [
    "# 기존 label\n",
    "print('기존 label:', train_dataset[100][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5133e304-ab0c-404f-b857-c7dae5994544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label을 one-hot encoding\n",
    "\n",
    "# features에 대한 transform.\n",
    "transform = transforms.ToTensor()\n",
    "# label에 대한 transform. one-hot encoding\n",
    "# PyTorch 연산 편의를 위해 int가 아닌 float로 변환해야됨\n",
    "target_transform = transforms.Lambda(\n",
    "    lambda y: torch.zeros(10, dtype=torch.float32).scatter_(\n",
    "        0, torch.tensor(y), value=1))\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                  train=True,\n",
    "                                                  download=True,\n",
    "                                                  transform=transform,\n",
    "                                                  target_transform=target_transform)\n",
    "\n",
    "# 위에서 이미 train dataset 다운했으므로 추가 다운로드 없이 기존 파일에서 transform진행\n",
    "# cf. https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#totensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b070d0-6c41-4c45-b8f3-9139c3f857d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 과정을 test_dataset에도 적용\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                  train=False,\n",
    "                                                  download=True,\n",
    "                                                  transform=transform,\n",
    "                                                  target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2ad6b-a1ef-48e5-bc64-1a625eff8abf",
   "metadata": {},
   "source": [
    "**cf. Lambda Transforms**  \n",
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor.  \n",
    "It first creates a zerp tensor of size 10(the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901f8e1-0344-4955-a2e4-76a221b0fd4d",
   "metadata": {},
   "source": [
    "**cf. `transforms.Lambda()`를 사용한 one-hot encoding 자세히 알아보기**  \n",
    "\n",
    "* 기존 label(=y)은 0 ~ 9의 int값을 가졌다는 것을 기억하자.\n",
    "\n",
    "* `import torchvision.transforms as transforms`  \n",
    "  dataset에 대한 여러 preprocessing을 지원한다.\n",
    "  \n",
    "* `transforms.Lambda()`  \n",
    "  user-defined lambda function을 tensor 객체에 적용할 수 있도록 해준다.\n",
    "\n",
    "* `lambda y: `\n",
    "  y(label, 0~9)를 입력받아서 이어지는 명령들을 수행한다.\n",
    "\n",
    "* `torch.zeros(10, dtype=torch.float32)`  \n",
    "  length가 10이고, dtype이 torch.float32인 tensor(array)를 만들어서 zeros로 채운다.\n",
    "  ```python\n",
    "  # cf.\n",
    "  torch.zeros(10, dtype=torch.float32)\n",
    "  > tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "  ```\n",
    "\n",
    "* `.scatter_(0, torch.tensor(y), value=1))`\n",
    "  - `torch.tensor(y)`: 입력받은 y(int)를 tensor로 바꾼다.\n",
    "    ```python\n",
    "    # cf.\n",
    "    y = 8\n",
    "    torch.tensor(y)\n",
    "    > tensor(8)\n",
    "    ```\n",
    "  - `Tensor.scatter(dim, index, src)`  \n",
    "    Tensor 객체(위에서 만든 `tensor([0., 0., ..., 0.])`을 의미)의 `dim`에 대해서 `index`에 해당하는 값을 `src` 값으로 덮어쓴다.  \n",
    "    (cf. PyTorch에서 dim=0이면 row, dim=1이면 column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e840276-8165-467f-9c28-d9780c2c30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoded label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "label mapping: tensor(8)\n"
     ]
    }
   ],
   "source": [
    "# cf. 위의 cf.를 참고할 것.\n",
    "# train_dataset[100][1] 즉, sample index가 100인 sample의 label(y)는 8(int)이었음.\n",
    "# transforms.Lambda()에 의해서 8번 째 index의 0만 1.(float)으로 덮어쓰고, 나머지는 0으로 유지.\n",
    "# 따라서 one-hot encoding됨.\n",
    "print('one-hot encoded label:', train_dataset[100][1])\n",
    "# one-hot encoded vector로부터 본래 label 추출\n",
    "print('label mapping:', torch.argmax(train_dataset[100][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb047038-196b-481c-8e97-93cfe97ff3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset을 dataloader에 batch size만큼씩 전달\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                            batch_size=600)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "#                                           batch_size=100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=600)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3b34cc-afe3-46cf-b11c-f3254dbdfc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# create the model(DenseLayerNeuralNetwork)\n",
    "class FashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # FashionDNN의 super class(= nn.Module)의 `__init__ method call\n",
    "        super(FashionDNN, self).__init__()\n",
    "        # pixel. 28 * 28 = 784\n",
    "        self.layer1 = nn.Linear(in_features=784, out_features=256)\n",
    "        # self.drop = nn.Dropout(0.25)\n",
    "        self.layer2 = nn.Linear(in_features=256, out_features=128)\n",
    "        # label. 0 ~ 9 총 10개\n",
    "        self.layer3 = nn.Linear(in_features=128, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# torch.device가 'cuda'여도 .to(torch.device) 안 해주면\n",
    "# 기본적으로 model은 cpu에 생성됨\n",
    "model = FashionDNN().to(torch.device)\n",
    "print(f'model running on {torch.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e437cb-eaf8-4a6f-94ec-9c9c54c18975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionDNN(\n",
       "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 확인\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4cfa79-ea16-4aa5-9144-d2078adba848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# cf. predict 예제\n",
    "X = torch.rand(1, 28, 28, device=torch.device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted class: {y_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "799cd476-484b-4c5f-b210-f8b408dbbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LossFunction and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b5f0d26-33f5-43dd-9ac8-10dc5f5985d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loader count [10], Loss: 0.9934\n",
      "Epoch [1/20], Loader count [20], Loss: 0.8144\n",
      "Epoch [1/20], Loader count [30], Loss: 0.6848\n",
      "Epoch [1/20], Loader count [40], Loss: 0.5502\n",
      "Epoch [1/20], Loader count [50], Loss: 0.5861\n",
      "Epoch [1/20], Loader count [60], Loss: 0.5128\n",
      "Epoch [1/20], Loader count [70], Loss: 0.4411\n",
      "Epoch [1/20], Loader count [80], Loss: 0.4424\n",
      "Epoch [1/20], Loader count [90], Loss: 0.5017\n",
      "Epoch [1/20], Loader count [100], Loss: 0.4013\n",
      "Epoch [1/20], Loss: 0.4013\n",
      "Epoch [2/20], Loader count [10], Loss: 0.4238\n",
      "Epoch [2/20], Loader count [20], Loss: 0.4375\n",
      "Epoch [2/20], Loader count [30], Loss: 0.4508\n",
      "Epoch [2/20], Loader count [40], Loss: 0.3936\n",
      "Epoch [2/20], Loader count [50], Loss: 0.4485\n",
      "Epoch [2/20], Loader count [60], Loss: 0.3930\n",
      "Epoch [2/20], Loader count [70], Loss: 0.3386\n",
      "Epoch [2/20], Loader count [80], Loss: 0.3629\n",
      "Epoch [2/20], Loader count [90], Loss: 0.3942\n",
      "Epoch [2/20], Loader count [100], Loss: 0.3594\n",
      "Epoch [2/20], Loss: 0.3594\n",
      "Epoch [3/20], Loader count [10], Loss: 0.3536\n",
      "Epoch [3/20], Loader count [20], Loss: 0.3771\n",
      "Epoch [3/20], Loader count [30], Loss: 0.3950\n",
      "Epoch [3/20], Loader count [40], Loss: 0.3340\n",
      "Epoch [3/20], Loader count [50], Loss: 0.4056\n",
      "Epoch [3/20], Loader count [60], Loss: 0.3566\n",
      "Epoch [3/20], Loader count [70], Loss: 0.3018\n",
      "Epoch [3/20], Loader count [80], Loss: 0.3356\n",
      "Epoch [3/20], Loader count [90], Loss: 0.3481\n",
      "Epoch [3/20], Loader count [100], Loss: 0.3242\n",
      "Epoch [3/20], Loss: 0.3242\n",
      "Epoch [4/20], Loader count [10], Loss: 0.3390\n",
      "Epoch [4/20], Loader count [20], Loss: 0.3390\n",
      "Epoch [4/20], Loader count [30], Loss: 0.3360\n",
      "Epoch [4/20], Loader count [40], Loss: 0.3066\n",
      "Epoch [4/20], Loader count [50], Loss: 0.3921\n",
      "Epoch [4/20], Loader count [60], Loss: 0.3082\n",
      "Epoch [4/20], Loader count [70], Loss: 0.2809\n",
      "Epoch [4/20], Loader count [80], Loss: 0.3024\n",
      "Epoch [4/20], Loader count [90], Loss: 0.3300\n",
      "Epoch [4/20], Loader count [100], Loss: 0.3013\n",
      "Epoch [4/20], Loss: 0.3013\n",
      "Epoch [5/20], Loader count [10], Loss: 0.3044\n",
      "Epoch [5/20], Loader count [20], Loss: 0.3231\n",
      "Epoch [5/20], Loader count [30], Loss: 0.3047\n",
      "Epoch [5/20], Loader count [40], Loss: 0.2728\n",
      "Epoch [5/20], Loader count [50], Loss: 0.3390\n",
      "Epoch [5/20], Loader count [60], Loss: 0.2911\n",
      "Epoch [5/20], Loader count [70], Loss: 0.2636\n",
      "Epoch [5/20], Loader count [80], Loss: 0.2716\n",
      "Epoch [5/20], Loader count [90], Loss: 0.3046\n",
      "Epoch [5/20], Loader count [100], Loss: 0.2993\n",
      "Epoch [5/20], Loss: 0.2993\n",
      "Epoch [6/20], Loader count [10], Loss: 0.2790\n",
      "Epoch [6/20], Loader count [20], Loss: 0.3002\n",
      "Epoch [6/20], Loader count [30], Loss: 0.3015\n",
      "Epoch [6/20], Loader count [40], Loss: 0.2557\n",
      "Epoch [6/20], Loader count [50], Loss: 0.3342\n",
      "Epoch [6/20], Loader count [60], Loss: 0.2821\n",
      "Epoch [6/20], Loader count [70], Loss: 0.2459\n",
      "Epoch [6/20], Loader count [80], Loss: 0.2577\n",
      "Epoch [6/20], Loader count [90], Loss: 0.2848\n",
      "Epoch [6/20], Loader count [100], Loss: 0.2884\n",
      "Epoch [6/20], Loss: 0.2884\n",
      "Epoch [7/20], Loader count [10], Loss: 0.2701\n",
      "Epoch [7/20], Loader count [20], Loss: 0.2889\n",
      "Epoch [7/20], Loader count [30], Loss: 0.2988\n",
      "Epoch [7/20], Loader count [40], Loss: 0.2565\n",
      "Epoch [7/20], Loader count [50], Loss: 0.3096\n",
      "Epoch [7/20], Loader count [60], Loss: 0.2510\n",
      "Epoch [7/20], Loader count [70], Loss: 0.2224\n",
      "Epoch [7/20], Loader count [80], Loss: 0.2676\n",
      "Epoch [7/20], Loader count [90], Loss: 0.2862\n",
      "Epoch [7/20], Loader count [100], Loss: 0.2958\n",
      "Epoch [7/20], Loss: 0.2958\n",
      "Epoch [8/20], Loader count [10], Loss: 0.2544\n",
      "Epoch [8/20], Loader count [20], Loss: 0.2756\n",
      "Epoch [8/20], Loader count [30], Loss: 0.2830\n",
      "Epoch [8/20], Loader count [40], Loss: 0.2569\n",
      "Epoch [8/20], Loader count [50], Loss: 0.3006\n",
      "Epoch [8/20], Loader count [60], Loss: 0.2436\n",
      "Epoch [8/20], Loader count [70], Loss: 0.2305\n",
      "Epoch [8/20], Loader count [80], Loss: 0.2420\n",
      "Epoch [8/20], Loader count [90], Loss: 0.2523\n",
      "Epoch [8/20], Loader count [100], Loss: 0.2968\n",
      "Epoch [8/20], Loss: 0.2968\n",
      "Epoch [9/20], Loader count [10], Loss: 0.2418\n",
      "Epoch [9/20], Loader count [20], Loss: 0.2629\n",
      "Epoch [9/20], Loader count [30], Loss: 0.2929\n",
      "Epoch [9/20], Loader count [40], Loss: 0.2525\n",
      "Epoch [9/20], Loader count [50], Loss: 0.3166\n",
      "Epoch [9/20], Loader count [60], Loss: 0.2439\n",
      "Epoch [9/20], Loader count [70], Loss: 0.2097\n",
      "Epoch [9/20], Loader count [80], Loss: 0.2237\n",
      "Epoch [9/20], Loader count [90], Loss: 0.2602\n",
      "Epoch [9/20], Loader count [100], Loss: 0.2786\n",
      "Epoch [9/20], Loss: 0.2786\n",
      "Epoch [10/20], Loader count [10], Loss: 0.2352\n",
      "Epoch [10/20], Loader count [20], Loss: 0.2602\n",
      "Epoch [10/20], Loader count [30], Loss: 0.2518\n",
      "Epoch [10/20], Loader count [40], Loss: 0.2409\n",
      "Epoch [10/20], Loader count [50], Loss: 0.2906\n",
      "Epoch [10/20], Loader count [60], Loss: 0.2356\n",
      "Epoch [10/20], Loader count [70], Loss: 0.1992\n",
      "Epoch [10/20], Loader count [80], Loss: 0.2193\n",
      "Epoch [10/20], Loader count [90], Loss: 0.2777\n",
      "Epoch [10/20], Loader count [100], Loss: 0.2792\n",
      "Epoch [10/20], Loss: 0.2792\n",
      "Epoch [11/20], Loader count [10], Loss: 0.2057\n",
      "Epoch [11/20], Loader count [20], Loss: 0.2483\n",
      "Epoch [11/20], Loader count [30], Loss: 0.2825\n",
      "Epoch [11/20], Loader count [40], Loss: 0.2275\n",
      "Epoch [11/20], Loader count [50], Loss: 0.2604\n",
      "Epoch [11/20], Loader count [60], Loss: 0.2465\n",
      "Epoch [11/20], Loader count [70], Loss: 0.2037\n",
      "Epoch [11/20], Loader count [80], Loss: 0.2116\n",
      "Epoch [11/20], Loader count [90], Loss: 0.2494\n",
      "Epoch [11/20], Loader count [100], Loss: 0.2821\n",
      "Epoch [11/20], Loss: 0.2821\n",
      "Epoch [12/20], Loader count [10], Loss: 0.2063\n",
      "Epoch [12/20], Loader count [20], Loss: 0.2348\n",
      "Epoch [12/20], Loader count [30], Loss: 0.2518\n",
      "Epoch [12/20], Loader count [40], Loss: 0.2203\n",
      "Epoch [12/20], Loader count [50], Loss: 0.2522\n",
      "Epoch [12/20], Loader count [60], Loss: 0.2267\n",
      "Epoch [12/20], Loader count [70], Loss: 0.2010\n",
      "Epoch [12/20], Loader count [80], Loss: 0.1986\n",
      "Epoch [12/20], Loader count [90], Loss: 0.2546\n",
      "Epoch [12/20], Loader count [100], Loss: 0.2506\n",
      "Epoch [12/20], Loss: 0.2506\n",
      "Epoch [13/20], Loader count [10], Loss: 0.1903\n",
      "Epoch [13/20], Loader count [20], Loss: 0.2318\n",
      "Epoch [13/20], Loader count [30], Loss: 0.2560\n",
      "Epoch [13/20], Loader count [40], Loss: 0.1892\n",
      "Epoch [13/20], Loader count [50], Loss: 0.2450\n",
      "Epoch [13/20], Loader count [60], Loss: 0.2243\n",
      "Epoch [13/20], Loader count [70], Loss: 0.1964\n",
      "Epoch [13/20], Loader count [80], Loss: 0.2011\n",
      "Epoch [13/20], Loader count [90], Loss: 0.2503\n",
      "Epoch [13/20], Loader count [100], Loss: 0.2506\n",
      "Epoch [13/20], Loss: 0.2506\n",
      "Epoch [14/20], Loader count [10], Loss: 0.1816\n",
      "Epoch [14/20], Loader count [20], Loss: 0.2279\n",
      "Epoch [14/20], Loader count [30], Loss: 0.2501\n",
      "Epoch [14/20], Loader count [40], Loss: 0.2191\n",
      "Epoch [14/20], Loader count [50], Loss: 0.2388\n",
      "Epoch [14/20], Loader count [60], Loss: 0.2254\n",
      "Epoch [14/20], Loader count [70], Loss: 0.1836\n",
      "Epoch [14/20], Loader count [80], Loss: 0.2031\n",
      "Epoch [14/20], Loader count [90], Loss: 0.2363\n",
      "Epoch [14/20], Loader count [100], Loss: 0.2634\n",
      "Epoch [14/20], Loss: 0.2634\n",
      "Epoch [15/20], Loader count [10], Loss: 0.1976\n",
      "Epoch [15/20], Loader count [20], Loss: 0.2339\n",
      "Epoch [15/20], Loader count [30], Loss: 0.2675\n",
      "Epoch [15/20], Loader count [40], Loss: 0.1942\n",
      "Epoch [15/20], Loader count [50], Loss: 0.2373\n",
      "Epoch [15/20], Loader count [60], Loss: 0.2297\n",
      "Epoch [15/20], Loader count [70], Loss: 0.1746\n",
      "Epoch [15/20], Loader count [80], Loss: 0.1979\n",
      "Epoch [15/20], Loader count [90], Loss: 0.2261\n",
      "Epoch [15/20], Loader count [100], Loss: 0.2428\n",
      "Epoch [15/20], Loss: 0.2428\n",
      "Epoch [16/20], Loader count [10], Loss: 0.2153\n",
      "Epoch [16/20], Loader count [20], Loss: 0.2359\n",
      "Epoch [16/20], Loader count [30], Loss: 0.2439\n",
      "Epoch [16/20], Loader count [40], Loss: 0.1751\n",
      "Epoch [16/20], Loader count [50], Loss: 0.2267\n",
      "Epoch [16/20], Loader count [60], Loss: 0.2377\n",
      "Epoch [16/20], Loader count [70], Loss: 0.2019\n",
      "Epoch [16/20], Loader count [80], Loss: 0.2021\n",
      "Epoch [16/20], Loader count [90], Loss: 0.2238\n",
      "Epoch [16/20], Loader count [100], Loss: 0.2430\n",
      "Epoch [16/20], Loss: 0.2430\n",
      "Epoch [17/20], Loader count [10], Loss: 0.1974\n",
      "Epoch [17/20], Loader count [20], Loss: 0.2218\n",
      "Epoch [17/20], Loader count [30], Loss: 0.2300\n",
      "Epoch [17/20], Loader count [40], Loss: 0.1912\n",
      "Epoch [17/20], Loader count [50], Loss: 0.2339\n",
      "Epoch [17/20], Loader count [60], Loss: 0.1964\n",
      "Epoch [17/20], Loader count [70], Loss: 0.1832\n",
      "Epoch [17/20], Loader count [80], Loss: 0.2034\n",
      "Epoch [17/20], Loader count [90], Loss: 0.2111\n",
      "Epoch [17/20], Loader count [100], Loss: 0.2249\n",
      "Epoch [17/20], Loss: 0.2249\n",
      "Epoch [18/20], Loader count [10], Loss: 0.1890\n",
      "Epoch [18/20], Loader count [20], Loss: 0.2191\n",
      "Epoch [18/20], Loader count [30], Loss: 0.2433\n",
      "Epoch [18/20], Loader count [40], Loss: 0.1964\n",
      "Epoch [18/20], Loader count [50], Loss: 0.2220\n",
      "Epoch [18/20], Loader count [60], Loss: 0.1806\n",
      "Epoch [18/20], Loader count [70], Loss: 0.1880\n",
      "Epoch [18/20], Loader count [80], Loss: 0.1917\n",
      "Epoch [18/20], Loader count [90], Loss: 0.2262\n",
      "Epoch [18/20], Loader count [100], Loss: 0.2376\n",
      "Epoch [18/20], Loss: 0.2376\n",
      "Epoch [19/20], Loader count [10], Loss: 0.2029\n",
      "Epoch [19/20], Loader count [20], Loss: 0.2491\n",
      "Epoch [19/20], Loader count [30], Loss: 0.2265\n",
      "Epoch [19/20], Loader count [40], Loss: 0.1708\n",
      "Epoch [19/20], Loader count [50], Loss: 0.2222\n",
      "Epoch [19/20], Loader count [60], Loss: 0.1924\n",
      "Epoch [19/20], Loader count [70], Loss: 0.1935\n",
      "Epoch [19/20], Loader count [80], Loss: 0.1839\n",
      "Epoch [19/20], Loader count [90], Loss: 0.2107\n",
      "Epoch [19/20], Loader count [100], Loss: 0.2344\n",
      "Epoch [19/20], Loss: 0.2344\n",
      "Epoch [20/20], Loader count [10], Loss: 0.1930\n",
      "Epoch [20/20], Loader count [20], Loss: 0.2064\n",
      "Epoch [20/20], Loader count [30], Loss: 0.2434\n",
      "Epoch [20/20], Loader count [40], Loss: 0.1827\n",
      "Epoch [20/20], Loader count [50], Loss: 0.2063\n",
      "Epoch [20/20], Loader count [60], Loss: 0.1591\n",
      "Epoch [20/20], Loader count [70], Loss: 0.1782\n",
      "Epoch [20/20], Loader count [80], Loss: 0.1806\n",
      "Epoch [20/20], Loader count [90], Loss: 0.2004\n",
      "Epoch [20/20], Loader count [100], Loss: 0.2334\n",
      "Epoch [20/20], Loss: 0.2334\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    count = 0\n",
    "    # images, labels 둘 다 tensor 객체\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(torch.device), labels.to(torch.device)\n",
    "\n",
    "        # train = Variable(images.view(100, 1, 28, 28))\n",
    "        # labels = Variable(labels)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if not (count % 10):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loader count [{count}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "276b11fa-c14b-4b21-a5a1-c1d7ba43d034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [1/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94         8\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.73      0.79      0.76        14\n",
      "           3       1.00      0.67      0.80         9\n",
      "           4       0.75      0.60      0.67        10\n",
      "           5       0.82      1.00      0.90         9\n",
      "           6       0.55      0.75      0.63         8\n",
      "           7       0.90      0.82      0.86        11\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       0.80      0.67      0.73         6\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.84      0.83      0.83       100\n",
      "weighted avg       0.85      0.84      0.84       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  6  2  0  1  0  0  0]\n",
      " [ 0  0  3  0  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  9  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  4]]\n",
      "\n",
      "\n",
      "<<<test count: [2/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.91      0.77      0.83        13\n",
      "           3       0.73      1.00      0.84         8\n",
      "           4       0.91      0.91      0.91        11\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.67      0.75      0.71         8\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.91      0.92      0.91       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[11  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  1  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [3/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.80      0.67      0.73        12\n",
      "           3       1.00      0.71      0.83         7\n",
      "           4       0.80      0.89      0.84         9\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.73      0.92      0.81        12\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.93      0.91      0.91       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[12  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  5  1  0  1  0  0  0]\n",
      " [ 0  0  1  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  1  0  0  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [4/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.90      0.60      0.72        15\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.82      0.69      0.75        13\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.47      0.82      0.60        11\n",
      "           7       0.92      1.00      0.96        11\n",
      "           8       1.00      0.80      0.89         5\n",
      "           9       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.88      0.85      0.86       100\n",
      "weighted avg       0.86      0.83      0.83       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  9  0  2  0  3  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  9  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [5/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.77      0.83        13\n",
      "           1       1.00      0.88      0.93         8\n",
      "           2       0.80      0.73      0.76        11\n",
      "           3       0.82      0.90      0.86        10\n",
      "           4       0.82      1.00      0.90        14\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.62      0.62      0.62         8\n",
      "           7       0.88      1.00      0.93         7\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n",
      "[[10  0  1  1  0  0  1  0  0  0]\n",
      " [ 0  7  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  1  0  2  0  0  0]\n",
      " [ 1  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 14  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  1  0  2  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [6/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.71      0.62         7\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      0.64      0.78        11\n",
      "           3       0.88      0.78      0.82         9\n",
      "           4       0.77      1.00      0.87        10\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.69      0.75      0.72        12\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.90      0.88      0.88       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  2  0  0  0]\n",
      " [ 1  0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 2  0  0  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 1  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [7/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        14\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       0.90      0.75      0.82        12\n",
      "           3       0.89      0.89      0.89         9\n",
      "           4       0.82      0.93      0.88        15\n",
      "           5       0.88      1.00      0.93         7\n",
      "           6       0.75      0.90      0.82        10\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      0.82      0.90        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.92      0.90      0.91       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[13  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  4  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  1  0  2  0  0  0]\n",
      " [ 0  0  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  1  0 14  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 1  0  0  0  0  1  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [8/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92        11\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       1.00      0.50      0.67         8\n",
      "           3       0.75      0.86      0.80         7\n",
      "           4       0.64      1.00      0.78         9\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       0.88      0.64      0.74        11\n",
      "           7       0.91      0.91      0.91        11\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.89      0.87      0.87       100\n",
      "weighted avg       0.90      0.88      0.88       100\n",
      "\n",
      "[[11  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  1  0  0  0  0  0  0]\n",
      " [ 1  0  4  0  2  0  1  0  0  0]\n",
      " [ 1  0  0  6  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  1  0  0]\n",
      " [ 0  0  0  1  3  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [9/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.88      0.88      0.88         8\n",
      "           3       0.92      1.00      0.96        11\n",
      "           4       0.89      0.80      0.84        10\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.60      0.75      0.67         4\n",
      "           7       1.00      0.93      0.96        14\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.93      0.93      0.93       100\n",
      "\n",
      "[[ 4  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  1  0  0  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [10/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.57      0.70        14\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.83      0.71      0.77         7\n",
      "           3       1.00      0.82      0.90        11\n",
      "           4       0.79      0.79      0.79        14\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.57      0.92      0.71        13\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.90      0.87      0.88       100\n",
      "weighted avg       0.88      0.85      0.85       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  6  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  2  0  0  0  0  0]\n",
      " [ 1  0  0  9  0  0  1  0  0  0]\n",
      " [ 0  0  1  0 11  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 0  0  0  0  1  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [11/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83         7\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       0.87      1.00      0.93        13\n",
      "           4       0.80      0.73      0.76        11\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       0.71      0.91      0.80        11\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       1.00      0.89      0.94         9\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.94      0.91      0.92       100\n",
      "weighted avg       0.93      0.92      0.92       100\n",
      "\n",
      "[[ 5  0  0  0  1  0  1  0  0  0]\n",
      " [ 0 10  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  8  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0]\n",
      " [ 0  0  0  0  1  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [12/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.44         5\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.90      0.75      0.82        12\n",
      "           3       0.67      1.00      0.80         4\n",
      "           4       0.75      0.80      0.77        15\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.69      0.69      0.69        16\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.85      0.86      0.85       100\n",
      "weighted avg       0.86      0.86      0.86       100\n",
      "\n",
      "[[ 2  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  2  0  1  0  0  0]\n",
      " [ 0  0  0  4  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 12  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 2  0  1  0  2  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [13/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80        16\n",
      "           1       0.89      1.00      0.94         8\n",
      "           2       0.91      0.67      0.77        15\n",
      "           3       0.70      1.00      0.82         7\n",
      "           4       0.76      0.93      0.84        14\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.62      0.62      0.62         8\n",
      "           7       1.00      1.00      1.00         4\n",
      "           8       1.00      0.86      0.92         7\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.87      0.88      0.87       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[12  0  1  2  0  0  1  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  4  0  1  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 13  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 2  0  0  1  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [14/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.62      0.67         8\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.83      0.42      0.56        12\n",
      "           3       0.91      0.83      0.87        12\n",
      "           4       0.75      0.80      0.77        15\n",
      "           5       0.88      1.00      0.93         7\n",
      "           6       0.50      0.89      0.64         9\n",
      "           7       1.00      1.00      1.00        10\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.86      0.85      0.84       100\n",
      "weighted avg       0.85      0.83      0.83       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  4  0  3  0  0  0]\n",
      " [ 1  0  0 10  0  0  1  0  0  0]\n",
      " [ 0  0  1  0 12  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [15/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        10\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      0.60      0.75        10\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       0.70      0.78      0.74         9\n",
      "           5       1.00      1.00      1.00        15\n",
      "           6       0.62      0.89      0.73         9\n",
      "           7       0.92      1.00      0.96        12\n",
      "           8       1.00      0.83      0.91         6\n",
      "           9       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.91      0.89      0.89       100\n",
      "weighted avg       0.92      0.90      0.90       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  2  0  2  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [16/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71         8\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       1.00      0.67      0.80        12\n",
      "           3       0.92      1.00      0.96        11\n",
      "           4       0.71      1.00      0.83         5\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.73      1.00      0.84         8\n",
      "           7       0.92      1.00      0.96        11\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.91      0.92      0.91       100\n",
      "weighted avg       0.93      0.92      0.92       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  2  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  8  0  2  0  1  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [17/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83         6\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.75      0.43      0.55         7\n",
      "           3       0.93      0.93      0.93        15\n",
      "           4       0.75      1.00      0.86        12\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.62      0.62      0.62         8\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.89      0.87      0.87       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  9  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  2  0  2  0  0  0]\n",
      " [ 0  0  0 14  1  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  1  0  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [18/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70         9\n",
      "           1       1.00      0.75      0.86         4\n",
      "           2       1.00      0.73      0.84        11\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       0.67      1.00      0.80         8\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.82      0.74      0.78        19\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.91      0.90      0.90       100\n",
      "weighted avg       0.91      0.89      0.89       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  3  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  8  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 4  0  0  0  1  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [19/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        13\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "           3       0.90      0.69      0.78        13\n",
      "           4       0.70      0.58      0.64        12\n",
      "           5       0.89      1.00      0.94         8\n",
      "           6       0.14      1.00      0.25         1\n",
      "           7       1.00      0.91      0.95        11\n",
      "           8       1.00      0.82      0.90        11\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.84      0.89      0.83       100\n",
      "weighted avg       0.91      0.87      0.88       100\n",
      "\n",
      "[[12  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  3  0  1  0  0  0]\n",
      " [ 0  0  1  1  7  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 10  0  0]\n",
      " [ 1  0  0  0  0  0  1  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [20/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        11\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.88      0.58      0.70        12\n",
      "           3       0.78      0.78      0.78         9\n",
      "           4       0.25      0.67      0.36         3\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       0.75      0.82      0.78        11\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.85      0.85      0.83       100\n",
      "weighted avg       0.90      0.86      0.87       100\n",
      "\n",
      "[[10  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 13  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  3  0  2  0  0  0]\n",
      " [ 0  0  0  7  1  0  1  0  0  0]\n",
      " [ 0  0  1  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  8]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [21/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.75      0.50      0.60        12\n",
      "           3       0.92      0.85      0.88        13\n",
      "           4       0.50      0.78      0.61         9\n",
      "           5       1.00      0.71      0.83         7\n",
      "           6       0.89      0.89      0.89         9\n",
      "           7       0.86      0.92      0.89        13\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.88      0.87      0.87       100\n",
      "weighted avg       0.88      0.86      0.86       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  5  0  1  0  0  0]\n",
      " [ 0  0  0 11  2  0  0  0  0  0]\n",
      " [ 0  0  1  1  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  2  0  0]\n",
      " [ 0  0  1  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [22/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       0.90      1.00      0.95         9\n",
      "           2       0.90      0.90      0.90        10\n",
      "           3       1.00      0.93      0.97        15\n",
      "           4       0.80      0.80      0.80        10\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.62      0.56      0.59         9\n",
      "           7       0.94      1.00      0.97        15\n",
      "           8       1.00      0.80      0.89         5\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.91      0.91      0.91       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  0  0  0  1  0  0]\n",
      " [ 0  0  0 14  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  8  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  1  0  2  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 15  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [23/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90        14\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.88      0.64      0.74        11\n",
      "           3       1.00      0.89      0.94         9\n",
      "           4       0.83      1.00      0.91        10\n",
      "           5       1.00      0.93      0.97        15\n",
      "           6       0.50      0.60      0.55         5\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[13  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  2  0  0  0]\n",
      " [ 0  0  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  1]\n",
      " [ 2  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [24/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.50      0.60      0.55         5\n",
      "           3       0.92      0.85      0.88        13\n",
      "           4       0.80      0.89      0.84         9\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.67      0.67      0.67         9\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      1.00      1.00        16\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.89      0.89      0.89       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[11  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  2  0  0  0  0  0]\n",
      " [ 0  0  1 11  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  2  1  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 16  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [25/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.80      0.80      0.80         5\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.86      0.86      0.86        14\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.67      0.67      0.67        12\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.91      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 10  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  1  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 2  0  1  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [26/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.58      0.67        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.82      0.75      0.78        12\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       0.60      0.75      0.67         8\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.44      0.50      0.47         8\n",
      "           7       0.86      0.86      0.86        14\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       0.86      0.86      0.86        14\n",
      "\n",
      "    accuracy                           0.80       100\n",
      "   macro avg       0.79      0.79      0.79       100\n",
      "weighted avg       0.81      0.80      0.80       100\n",
      "\n",
      "[[ 7  0  1  0  0  0  3  0  1  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  1  0  2  0  0  0]\n",
      " [ 0  0  0  3  1  0  0  0  0  0]\n",
      " [ 0  0  1  1  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 2  0  0  0  2  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  2]\n",
      " [ 0  0  0  0  0  1  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  2  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [27/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        11\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.64      0.78      0.70         9\n",
      "           3       0.78      0.64      0.70        11\n",
      "           4       0.87      0.76      0.81        17\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       0.65      0.92      0.76        12\n",
      "           7       1.00      1.00      1.00        12\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.88      0.87      0.87       100\n",
      "weighted avg       0.87      0.85      0.85       100\n",
      "\n",
      "[[ 8  0  3  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  1  0  1  0  0  0]\n",
      " [ 1  0  0  7  1  0  2  0  0  0]\n",
      " [ 0  0  1  1 13  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  0]\n",
      " [ 0  0  0  1  0  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]]\n",
      "\n",
      "\n",
      "<<<test count: [28/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92         7\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.57      0.67      0.62         6\n",
      "           3       0.91      0.91      0.91        11\n",
      "           4       0.92      0.73      0.81        15\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       0.64      0.88      0.74         8\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       1.00      1.00      1.00        15\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  1  0  1  0  0  0]\n",
      " [ 0  0  0 10  0  0  1  0  0  0]\n",
      " [ 0  0  2  1 11  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [29/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86        13\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       0.90      0.82      0.86        11\n",
      "           3       1.00      0.86      0.92        14\n",
      "           4       0.38      0.75      0.50         4\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.90      0.60      0.72        15\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       0.89      1.00      0.94         8\n",
      "           9       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.88      0.89      0.88       100\n",
      "weighted avg       0.90      0.88      0.88       100\n",
      "\n",
      "[[12  0  0  0  0  0  0  0  1  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  1  0  1  0  0  0]\n",
      " [ 0  1  0 12  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 3  0  0  0  3  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [30/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.83      0.67      0.74        15\n",
      "           3       0.80      0.67      0.73         6\n",
      "           4       0.57      0.89      0.70         9\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       0.67      0.71      0.69        14\n",
      "           7       0.88      0.78      0.82         9\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.83      0.82      0.82       100\n",
      "weighted avg       0.83      0.81      0.81       100\n",
      "\n",
      "[[ 9  0  1  0  0  0  1  0  0  0]\n",
      " [ 0 11  0  1  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  2  0  3  0  0  0]\n",
      " [ 1  0  0  4  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  0]\n",
      " [ 1  0  1  0  2  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  2]\n",
      " [ 0  0  0  0  1  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [31/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      0.75      0.86        12\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       0.62      0.71      0.67         7\n",
      "           5       0.90      0.82      0.86        11\n",
      "           6       0.29      0.50      0.36         4\n",
      "           7       0.81      0.93      0.87        14\n",
      "           8       1.00      0.88      0.93        16\n",
      "           9       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.80      0.79      0.79       100\n",
      "weighted avg       0.86      0.83      0.84       100\n",
      "\n",
      "[[ 4  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  2  0  1  0  0  0]\n",
      " [ 0  0  0 11  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  5  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  2]\n",
      " [ 1  0  0  0  1  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 13  0  0]\n",
      " [ 1  0  0  0  0  0  0  1 14  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [32/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        10\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.80      1.00      0.89         8\n",
      "           3       1.00      0.78      0.88         9\n",
      "           4       0.73      0.89      0.80         9\n",
      "           5       1.00      0.83      0.91         6\n",
      "           6       0.92      0.80      0.86        15\n",
      "           7       0.89      1.00      0.94        16\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.92      0.91      0.91       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[ 9  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  1]\n",
      " [ 0  0  2  0  1  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 16  0  0]\n",
      " [ 0  0  0  0  0  0  0  1 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [33/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64        13\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.82      0.60      0.69        15\n",
      "           3       0.50      0.71      0.59         7\n",
      "           4       0.64      0.82      0.72        11\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.43      0.43      0.43         7\n",
      "           7       1.00      0.93      0.96        14\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.79       100\n",
      "   macro avg       0.79      0.80      0.79       100\n",
      "weighted avg       0.81      0.79      0.79       100\n",
      "\n",
      "[[ 8  0  0  2  0  0  3  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  2  3  0  1  0  0  0]\n",
      " [ 1  0  1  5  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 3  0  0  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  1]\n",
      " [ 0  0  0  0  1  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [34/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        14\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.78      0.78      0.78         9\n",
      "           3       0.88      0.88      0.88         8\n",
      "           4       0.56      1.00      0.71         5\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.67      0.67      0.67         9\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      0.94      0.97        16\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.89      0.90      0.89       100\n",
      "weighted avg       0.92      0.90      0.90       100\n",
      "\n",
      "[[11  0  1  0  0  0  2  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  0  0  0  0]\n",
      " [ 0  0  1  7  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  1  2  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 15  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [35/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75         9\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.83      0.71      0.77        14\n",
      "           3       0.79      0.92      0.85        12\n",
      "           4       0.75      0.90      0.82        10\n",
      "           5       0.91      0.77      0.83        13\n",
      "           6       0.75      0.60      0.67        10\n",
      "           7       0.92      1.00      0.96        12\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       0.67      1.00      0.80         6\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.84      0.84      0.83       100\n",
      "weighted avg       0.84      0.83      0.83       100\n",
      "\n",
      "[[ 6  0  0  0  0  1  1  0  1  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  1  2  0  1  0  0  0]\n",
      " [ 0  0  0 11  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  3]\n",
      " [ 1  0  1  2  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [36/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.77      0.83        13\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.83      0.38      0.53        13\n",
      "           3       0.88      1.00      0.93         7\n",
      "           4       0.57      0.80      0.67         5\n",
      "           5       0.93      1.00      0.96        13\n",
      "           6       0.45      1.00      0.62         5\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.86      0.89      0.85       100\n",
      "weighted avg       0.90      0.87      0.87       100\n",
      "\n",
      "[[10  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  5  0  3  0  4  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [37/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88         8\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.88      1.00      0.93         7\n",
      "           3       0.91      1.00      0.95        10\n",
      "           4       0.85      0.92      0.88        12\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.86      0.67      0.75         9\n",
      "           7       0.90      0.90      0.90        10\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       0.87      0.93      0.90        14\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.91      0.91      0.91       100\n",
      "weighted avg       0.91      0.91      0.91       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 1  0  0  0  2  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  1]\n",
      " [ 0  0  1  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [38/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.67      0.89      0.76         9\n",
      "           3       1.00      0.82      0.90        11\n",
      "           4       0.77      0.83      0.80        12\n",
      "           5       0.67      0.67      0.67         3\n",
      "           6       0.79      0.92      0.85        12\n",
      "           7       0.90      0.90      0.90        10\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       0.91      0.91      0.91        11\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.86      0.85      0.85       100\n",
      "weighted avg       0.88      0.86      0.86       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  9  2  0  0  0  0  0]\n",
      " [ 0  0  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  1]\n",
      " [ 1  0  0  0  0  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  9  0  0]\n",
      " [ 0  0  2  0  0  0  1  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [39/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.60      0.50      0.55         6\n",
      "           3       0.67      0.89      0.76         9\n",
      "           4       0.57      0.80      0.67        10\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.80      0.36      0.50        11\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       0.91      0.91      0.91        11\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.84      0.84      0.83       100\n",
      "weighted avg       0.86      0.85      0.84       100\n",
      "\n",
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  1  2  0  0  0  0  0]\n",
      " [ 0  0  0  8  1  0  0  0  0  0]\n",
      " [ 0  0  0  1  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 0  0  2  2  3  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [40/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.93      0.93      0.93        15\n",
      "           2       0.67      0.60      0.63        10\n",
      "           3       0.92      0.92      0.92        13\n",
      "           4       0.75      1.00      0.86         9\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.64      0.88      0.74         8\n",
      "           7       0.89      1.00      0.94         8\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.88      0.87      0.86       100\n",
      "weighted avg       0.89      0.87      0.87       100\n",
      "\n",
      "[[ 7  0  2  0  0  0  1  0  0  0]\n",
      " [ 0 14  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  1  0  3  0  0  0]\n",
      " [ 0  0  0 12  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  1  0  0  0  0  0 10  0]\n",
      " [ 0  1  0  0  0  0  0  1  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [41/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        13\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.75      0.55      0.63        11\n",
      "           3       0.83      0.71      0.77         7\n",
      "           4       0.64      0.75      0.69        12\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.70      0.88      0.78         8\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.87      0.86      0.86       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[12  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  1  3  0  1  0  0  0]\n",
      " [ 1  0  0  5  1  0  0  0  0  0]\n",
      " [ 0  0  2  0  9  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 1  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [42/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.83      0.69        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.85      0.65      0.73        17\n",
      "           3       0.83      0.91      0.87        11\n",
      "           4       0.62      0.83      0.71         6\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.50      0.36      0.42        14\n",
      "           7       1.00      0.75      0.86         4\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.80       100\n",
      "   macro avg       0.82      0.83      0.82       100\n",
      "weighted avg       0.81      0.80      0.79       100\n",
      "\n",
      "[[10  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 11  0  2  0  3  0  0  0]\n",
      " [ 0  0  0 10  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0]\n",
      " [ 6  0  1  1  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [43/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85        12\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.86      0.95      0.90        19\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       0.75      0.50      0.60         6\n",
      "           5       0.88      1.00      0.93         7\n",
      "           6       0.69      0.69      0.69        13\n",
      "           7       1.00      0.91      0.95        11\n",
      "           8       1.00      0.85      0.92        13\n",
      "           9       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.90      0.88      0.88       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n",
      "[[11  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 18  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  6  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  3  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 3  0  1  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 10  0  0]\n",
      " [ 0  0  0  0  1  0  1  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [44/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92         7\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.89      0.89      0.89         9\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       1.00      0.92      0.96        13\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.71      0.83      0.77         6\n",
      "           7       1.00      1.00      1.00        12\n",
      "           8       0.92      1.00      0.96        12\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.94      0.94      0.94       100\n",
      "weighted avg       0.95      0.95      0.95       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  1  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  1  0]\n",
      " [ 0  0  0  1 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [45/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92         7\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.80      1.00      0.89         4\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.92      0.86      0.89        14\n",
      "           5       1.00      0.89      0.94         9\n",
      "           6       0.70      0.78      0.74         9\n",
      "           7       0.86      1.00      0.92         6\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.92      0.93      0.92       100\n",
      "weighted avg       0.94      0.93      0.93       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 12  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  1  0  0]\n",
      " [ 0  0  1  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [46/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        10\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "           3       0.86      1.00      0.92         6\n",
      "           4       1.00      0.86      0.92        14\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       0.75      0.82      0.78        11\n",
      "           7       0.83      1.00      0.91        10\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       0.90      0.82      0.86        11\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.92      0.92      0.92       100\n",
      "weighted avg       0.93      0.92      0.92       100\n",
      "\n",
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 14  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  6  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  1]\n",
      " [ 1  0  0  1  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [47/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78        10\n",
      "           1       1.00      0.88      0.93         8\n",
      "           2       0.62      0.38      0.48        13\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       0.64      0.70      0.67        10\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.46      0.75      0.57         8\n",
      "           7       1.00      0.88      0.93         8\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.83      0.82      0.81       100\n",
      "weighted avg       0.82      0.81      0.81       100\n",
      "\n",
      "[[ 7  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  7  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  4  0  4  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  7  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  1]\n",
      " [ 0  0  0  0  0  1  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [48/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.75      0.50      0.60         6\n",
      "           3       0.93      0.93      0.93        15\n",
      "           4       0.82      0.93      0.88        15\n",
      "           5       1.00      0.93      0.97        15\n",
      "           6       0.60      0.75      0.67         8\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      0.89      0.94         9\n",
      "           9       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.87      0.86      0.86       100\n",
      "weighted avg       0.89      0.88      0.88       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  2  0  1  0  0  0]\n",
      " [ 0  0  0 14  1  0  0  0  0  0]\n",
      " [ 0  0  0  0 14  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  1]\n",
      " [ 1  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  4]]\n",
      "\n",
      "\n",
      "<<<test count: [49/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         6\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.90      0.60      0.72        15\n",
      "           3       1.00      0.86      0.92         7\n",
      "           4       0.55      0.86      0.67         7\n",
      "           5       0.92      0.92      0.92        13\n",
      "           6       0.70      0.64      0.67        11\n",
      "           7       0.80      0.89      0.84         9\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.85      0.86      0.84       100\n",
      "weighted avg       0.86      0.84      0.84       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  4  0  2  0  0  0]\n",
      " [ 0  0  0  6  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  1]\n",
      " [ 3  0  0  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  1 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 14]]\n",
      "\n",
      "\n",
      "<<<test count: [50/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        13\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.67      0.67      0.67         6\n",
      "           3       0.79      0.94      0.86        16\n",
      "           4       0.88      0.70      0.78        10\n",
      "           5       1.00      0.88      0.93         8\n",
      "           6       0.75      1.00      0.86         6\n",
      "           7       1.00      0.86      0.92         7\n",
      "           8       1.00      0.93      0.97        15\n",
      "           9       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.90      0.89      0.89       100\n",
      "\n",
      "[[11  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  1  0  1  0  0  0]\n",
      " [ 0  0  1 15  0  0  0  0  0  0]\n",
      " [ 0  0  1  2  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  1  0  0  0  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [51/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71        11\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.78      0.70      0.74        10\n",
      "           3       0.87      0.93      0.90        14\n",
      "           4       0.67      0.67      0.67         9\n",
      "           5       0.93      0.88      0.90        16\n",
      "           6       0.59      0.83      0.69        12\n",
      "           7       0.78      0.88      0.82         8\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       0.83      0.83      0.83        12\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.84      0.83      0.83       100\n",
      "weighted avg       0.83      0.81      0.81       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  5  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  1  0  0  0]\n",
      " [ 0  0  0 13  0  0  1  0  0  0]\n",
      " [ 0  0  2  1  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  2]\n",
      " [ 0  0  0  1  1  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  2  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [52/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88         9\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.78      0.88      0.82         8\n",
      "           3       0.86      0.86      0.86         7\n",
      "           4       0.83      0.83      0.83        18\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.53      0.62      0.57        13\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.90      0.89      0.89       100\n",
      "weighted avg       0.88      0.87      0.87       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  6  0  0  1  0  0  0]\n",
      " [ 0  0  0  1 15  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  2  0  3  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [53/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        13\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.80      0.57      0.67         7\n",
      "           3       0.88      0.88      0.88        16\n",
      "           4       0.75      0.90      0.82        10\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.70      0.70      0.70        10\n",
      "           7       1.00      0.91      0.95        11\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.90      0.89      0.89       100\n",
      "weighted avg       0.89      0.89      0.89       100\n",
      "\n",
      "[[12  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  2  0  1  0  0  0]\n",
      " [ 0  0  0 14  1  0  1  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 1  0  0  2  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [54/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       1.00      0.86      0.92         7\n",
      "           2       0.67      0.67      0.67         3\n",
      "           3       0.67      0.80      0.73         5\n",
      "           4       0.80      0.80      0.80        15\n",
      "           5       0.91      1.00      0.95        10\n",
      "           6       0.44      0.80      0.57         5\n",
      "           7       1.00      0.93      0.97        15\n",
      "           8       1.00      0.88      0.94        17\n",
      "           9       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.84      0.84      0.83       100\n",
      "weighted avg       0.90      0.87      0.88       100\n",
      "\n",
      "[[ 7  0  0  1  0  1  1  0  0  0]\n",
      " [ 0  6  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  4  1  0  0  0  0  0]\n",
      " [ 0  0  1  0 12  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  1]\n",
      " [ 0  0  0  0  1  0  1  0 15  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [55/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.86      0.75      0.80        16\n",
      "           3       0.83      0.83      0.83         6\n",
      "           4       0.75      0.90      0.82        10\n",
      "           5       1.00      0.93      0.96        14\n",
      "           6       0.62      0.80      0.70        10\n",
      "           7       0.89      0.89      0.89         9\n",
      "           8       1.00      0.67      0.80         6\n",
      "           9       0.88      0.94      0.91        16\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.86      0.83      0.84       100\n",
      "weighted avg       0.86      0.85      0.85       100\n",
      "\n",
      "[[ 3  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  2  0  2  0  0  0]\n",
      " [ 0  0  0  5  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  1]\n",
      " [ 1  0  0  1  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  1]\n",
      " [ 0  0  1  0  1  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [56/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        10\n",
      "           1       1.00      0.87      0.93        15\n",
      "           2       0.80      0.80      0.80        10\n",
      "           3       0.75      0.75      0.75        12\n",
      "           4       0.50      0.71      0.59         7\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       0.69      0.69      0.69        13\n",
      "           7       0.91      0.91      0.91        11\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.83      0.82      0.82       100\n",
      "weighted avg       0.83      0.82      0.83       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 13  0  1  1  0  0  0  0  0]\n",
      " [ 0  0  8  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  9  2  0  1  0  0  0]\n",
      " [ 0  0  1  0  5  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  1]\n",
      " [ 1  0  1  2  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [57/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      0.82      0.90        11\n",
      "           3       1.00      1.00      1.00        19\n",
      "           4       0.83      0.71      0.77         7\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.75      0.75      0.75        12\n",
      "           7       0.86      1.00      0.92         6\n",
      "           8       0.90      1.00      0.95         9\n",
      "           9       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.91      0.91      0.91       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  1  0  1  0  0  0]\n",
      " [ 0  0  0 19  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  1  0  1  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 3  0  0  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [58/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80         8\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.80      0.67      0.73        12\n",
      "           3       0.90      1.00      0.95         9\n",
      "           4       0.83      0.91      0.87        11\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.75      0.86      0.80         7\n",
      "           7       1.00      1.00      1.00        10\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.91      0.91      0.91       100\n",
      "weighted avg       0.91      0.91      0.91       100\n",
      "\n",
      "[[ 6  0  1  1  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  8  0  2  0  1  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 0  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [59/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.78      0.64      0.70        11\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       0.82      0.82      0.82        11\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.73      0.85      0.79        13\n",
      "           7       0.93      1.00      0.97        14\n",
      "           8       1.00      0.86      0.92         7\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.91      0.91      0.91       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  1  0  0]\n",
      " [ 1  0  0  0  1  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [60/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        12\n",
      "           1       0.89      1.00      0.94         8\n",
      "           2       1.00      0.60      0.75        10\n",
      "           3       0.83      0.71      0.77        14\n",
      "           4       0.46      1.00      0.63         6\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.67      0.80      0.73        10\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.87      0.86      0.84       100\n",
      "weighted avg       0.88      0.84      0.85       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  3  0  1  0  0  0]\n",
      " [ 0  1  0 10  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  9  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [61/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      0.83      0.91        12\n",
      "           3       0.80      0.80      0.80         5\n",
      "           4       0.71      0.83      0.77         6\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.67      0.80      0.73        10\n",
      "           7       1.00      0.86      0.92         7\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.89      0.87      0.88       100\n",
      "weighted avg       0.90      0.89      0.89       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  4  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  5  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  1]\n",
      " [ 2  0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  1  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [62/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.94      1.00      0.97        16\n",
      "           2       0.75      0.75      0.75         8\n",
      "           3       1.00      0.90      0.95        10\n",
      "           4       0.67      0.80      0.73        10\n",
      "           5       1.00      0.93      0.96        14\n",
      "           6       0.86      0.67      0.75         9\n",
      "           7       0.86      0.86      0.86         7\n",
      "           8       1.00      0.86      0.92         7\n",
      "           9       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.88      0.88      0.87       100\n",
      "weighted avg       0.90      0.89      0.89       100\n",
      "\n",
      "[[ 4  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  6  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  9  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  1  0  0]\n",
      " [ 1  0  1  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  0  1  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [63/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77         7\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.81      0.72      0.76        18\n",
      "           3       0.93      0.87      0.90        15\n",
      "           4       0.69      0.92      0.79        12\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.60      0.75      0.67        12\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      0.50      0.67         6\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.89      0.85      0.85       100\n",
      "weighted avg       0.86      0.84      0.84       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 13  0  2  0  3  0  0  0]\n",
      " [ 0  0  0 13  2  0  0  0  0  0]\n",
      " [ 0  0  1  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 1  0  0  1  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  2  0  0  0  1  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [64/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.82      1.00      0.90         9\n",
      "           3       1.00      1.00      1.00        13\n",
      "           4       1.00      1.00      1.00         6\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.90      0.75      0.82        12\n",
      "           7       1.00      1.00      1.00        10\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.94       100\n",
      "   macro avg       0.93      0.94      0.94       100\n",
      "weighted avg       0.95      0.94      0.94       100\n",
      "\n",
      "[[ 5  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 2  0  1  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [65/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       0.89      0.89      0.89         9\n",
      "           3       0.73      0.80      0.76        10\n",
      "           4       0.83      0.77      0.80        13\n",
      "           5       0.91      1.00      0.95        10\n",
      "           6       0.56      0.62      0.59         8\n",
      "           7       1.00      0.91      0.95        11\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.88      0.87      0.87       100\n",
      "\n",
      "[[ 7  0  0  0  1  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  1  0  0  0  0  0  0]\n",
      " [ 0  1  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  0  1 10  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  1  0  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 10  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [66/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.82      0.75      0.78        12\n",
      "           3       1.00      0.86      0.92         7\n",
      "           4       0.45      0.56      0.50         9\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.74      0.74      0.74        19\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       0.92      0.92      0.92        13\n",
      "           9       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.87      0.86      0.86       100\n",
      "weighted avg       0.85      0.84      0.84       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  2  0  1  0  0  0]\n",
      " [ 0  0  0  6  1  0  0  0  0  0]\n",
      " [ 0  0  2  0  5  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 1  0  0  0  3  0 14  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  1 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [67/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67        10\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.89      0.89      0.89         9\n",
      "           3       0.89      0.80      0.84        10\n",
      "           4       0.62      1.00      0.77         5\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.57      0.67      0.62        12\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       1.00      0.88      0.93        16\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.86      0.87      0.86       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[ 6  0  0  1  0  0  3  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  8  1  0  1  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  1  0  0]\n",
      " [ 2  0  1  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  2  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [68/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        13\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      0.86      0.92         7\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.91      1.00      0.95        10\n",
      "           5       1.00      0.83      0.91        12\n",
      "           6       0.80      0.80      0.80         5\n",
      "           7       0.85      0.92      0.88        12\n",
      "           8       0.83      1.00      0.91         5\n",
      "           9       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.93      0.93      0.92       100\n",
      "weighted avg       0.93      0.93      0.93       100\n",
      "\n",
      "[[12  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  1  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  1  1  0]\n",
      " [ 1  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 16]]\n",
      "\n",
      "\n",
      "<<<test count: [69/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.75      0.50      0.60        12\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.56      0.62      0.59         8\n",
      "           5       1.00      0.89      0.94         9\n",
      "           6       0.58      0.78      0.67         9\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.87      0.87      0.86       100\n",
      "weighted avg       0.88      0.87      0.87       100\n",
      "\n",
      "[[ 9  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  3  0  3  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  5  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  1]\n",
      " [ 0  0  0  1  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [70/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82         8\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      0.75      0.86        12\n",
      "           3       1.00      0.75      0.86         8\n",
      "           4       0.62      0.62      0.62         8\n",
      "           5       1.00      1.00      1.00        15\n",
      "           6       0.62      0.89      0.73         9\n",
      "           7       0.92      1.00      0.96        11\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.91      0.89      0.89       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  2  0  1  0  0  0]\n",
      " [ 1  0  0  6  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  3  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [71/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82         8\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.92      0.75      0.83        16\n",
      "           3       1.00      1.00      1.00         9\n",
      "           4       0.50      0.83      0.62         6\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.89      0.73      0.80        11\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.91      0.90       100\n",
      "weighted avg       0.92      0.90      0.90       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 2  0  0  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 14]]\n",
      "\n",
      "\n",
      "<<<test count: [72/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84        14\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.67      0.75      0.71         8\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.78      1.00      0.88         7\n",
      "           5       1.00      0.78      0.88         9\n",
      "           6       1.00      0.53      0.70        15\n",
      "           7       0.67      1.00      0.80         6\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.87      0.88      0.86       100\n",
      "weighted avg       0.88      0.86      0.86       100\n",
      "\n",
      "[[13  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  1  1  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  1  0  1]\n",
      " [ 4  0  2  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  2  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [73/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.78      0.64      0.70        11\n",
      "           3       0.89      0.89      0.89         9\n",
      "           4       0.60      0.60      0.60        10\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.50      0.71      0.59         7\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.88      0.88      0.87       100\n",
      "weighted avg       0.89      0.88      0.88       100\n",
      "\n",
      "[[11  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  3  0  1  0  0  0]\n",
      " [ 0  0  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  6  0  3  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0]\n",
      " [ 0  0  1  0  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [74/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.83      1.00      0.91         5\n",
      "           2       0.83      0.71      0.77        14\n",
      "           3       1.00      0.76      0.87        17\n",
      "           4       0.76      0.87      0.81        15\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.64      0.88      0.74         8\n",
      "           7       1.00      1.00      1.00         4\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.87      0.88      0.87       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[ 4  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 10  0  1  0  2  0  0  0]\n",
      " [ 0  1  0 13  3  0  0  0  0  0]\n",
      " [ 0  0  2  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  1]\n",
      " [ 1  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [75/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86         7\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.88      0.78      0.82         9\n",
      "           3       0.90      1.00      0.95         9\n",
      "           4       0.90      0.90      0.90        10\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.92      0.92      0.92        12\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.94       100\n",
      "   macro avg       0.94      0.94      0.94       100\n",
      "weighted avg       0.94      0.94      0.94       100\n",
      "\n",
      "[[ 6  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  1]\n",
      " [ 1  0  0  0  0  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [76/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       1.00      0.80      0.89        10\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       0.94      0.83      0.88        18\n",
      "           4       0.94      0.94      0.94        16\n",
      "           5       1.00      0.75      0.86         8\n",
      "           6       0.55      0.86      0.67         7\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.88      1.00      0.94        15\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.92      0.88      0.89       100\n",
      "weighted avg       0.92      0.90      0.90       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  1  1  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  1  0  0  0]\n",
      " [ 0  0  0 15  0  0  3  0  0  0]\n",
      " [ 0  0  0  0 15  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  2]\n",
      " [ 1  0  0  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [77/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      0.85      0.92        13\n",
      "           3       1.00      0.62      0.76        13\n",
      "           4       0.56      1.00      0.71         5\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.70      1.00      0.82         7\n",
      "           7       1.00      0.92      0.96        12\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.92      0.94      0.91       100\n",
      "weighted avg       0.95      0.92      0.92       100\n",
      "\n",
      "[[12  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  8  2  0  3  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [78/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90         9\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       1.00      0.78      0.88         9\n",
      "           3       0.90      0.90      0.90        10\n",
      "           4       0.88      0.88      0.88         8\n",
      "           5       1.00      0.88      0.93        16\n",
      "           6       0.62      0.62      0.62         8\n",
      "           7       0.94      0.89      0.91        18\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       0.73      1.00      0.84         8\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.89      0.89      0.89       100\n",
      "weighted avg       0.90      0.89      0.89       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  9  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  7  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  1  0  1]\n",
      " [ 2  0  0  1  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 16  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [79/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.54      0.67        13\n",
      "           1       1.00      0.86      0.92         7\n",
      "           2       0.71      0.71      0.71         7\n",
      "           3       1.00      0.85      0.92        13\n",
      "           4       0.70      1.00      0.82         7\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.67      0.91      0.77        11\n",
      "           7       1.00      0.93      0.96        14\n",
      "           8       0.88      1.00      0.93         7\n",
      "           9       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.87      0.88      0.87       100\n",
      "weighted avg       0.89      0.87      0.87       100\n",
      "\n",
      "[[ 7  0  1  0  0  0  4  0  1  0]\n",
      " [ 0  6  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  5  0  2  0  0  0  0  0]\n",
      " [ 0  0  1 11  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 1  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [80/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82        10\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.86      0.75      0.80         8\n",
      "           3       0.78      0.88      0.82         8\n",
      "           4       0.72      0.93      0.81        14\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       1.00      0.58      0.74        12\n",
      "           7       0.91      0.91      0.91        11\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.88      0.86      0.86       100\n",
      "weighted avg       0.88      0.86      0.86       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  7  1  0  0  0  0  0]\n",
      " [ 0  0  0  1 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  1]\n",
      " [ 2  0  1  0  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  1]\n",
      " [ 1  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [81/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        18\n",
      "           1       1.00      0.83      0.91         6\n",
      "           2       1.00      0.45      0.62        11\n",
      "           3       0.80      0.89      0.84         9\n",
      "           4       0.50      0.86      0.63         7\n",
      "           5       0.92      0.92      0.92        12\n",
      "           6       0.70      0.78      0.74         9\n",
      "           7       1.00      0.83      0.91         6\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.88      1.00      0.93        14\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.87      0.84      0.84       100\n",
      "weighted avg       0.88      0.85      0.85       100\n",
      "\n",
      "[[17  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  5  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  4  0  2  0  0  0]\n",
      " [ 0  0  0  8  1  0  0  0  0  0]\n",
      " [ 0  0  0  1  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 1  0  0  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  1]\n",
      " [ 0  0  0  0  0  1  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]]\n",
      "\n",
      "\n",
      "<<<test count: [82/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83         6\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.57      0.67      0.62         6\n",
      "           3       0.90      1.00      0.95         9\n",
      "           4       0.79      0.85      0.81        13\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       1.00      0.90      0.95        10\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.91      0.91      0.91       100\n",
      "weighted avg       0.93      0.92      0.92       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  9  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  2  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [83/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88         7\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       0.92      0.92      0.92        12\n",
      "           3       1.00      0.78      0.88         9\n",
      "           4       0.78      0.88      0.82         8\n",
      "           5       0.93      0.88      0.90        16\n",
      "           6       0.85      0.73      0.79        15\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.87      0.88      0.87       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  1  0  0  0]\n",
      " [ 1  0  0  7  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  7  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  1  0  1]\n",
      " [ 1  1  0  0  2  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  3]]\n",
      "\n",
      "\n",
      "<<<test count: [84/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.80      1.00      0.89         4\n",
      "           3       0.78      0.88      0.82         8\n",
      "           4       1.00      1.00      1.00         9\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.75      0.86      0.80         7\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.93      0.95      0.94       100\n",
      "weighted avg       0.96      0.95      0.95       100\n",
      "\n",
      "[[12  0  0  2  0  0  1  0  0  0]\n",
      " [ 0 15  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [85/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      0.62      0.77         8\n",
      "           3       0.88      1.00      0.93         7\n",
      "           4       0.92      0.79      0.85        14\n",
      "           5       1.00      0.89      0.94         9\n",
      "           6       0.50      1.00      0.67         6\n",
      "           7       0.92      1.00      0.96        12\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.92      0.91      0.90       100\n",
      "weighted avg       0.94      0.91      0.91       100\n",
      "\n",
      "[[10  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  1  0  2  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [86/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       1.00      0.73      0.84        11\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       0.80      0.73      0.76        11\n",
      "           5       0.86      0.86      0.86         7\n",
      "           6       0.62      0.80      0.70        10\n",
      "           7       0.86      0.92      0.89        13\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.89      0.88      0.88       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  1  0  2  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  1  0  0]\n",
      " [ 1  0  0  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [87/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83        11\n",
      "           1       0.94      1.00      0.97        16\n",
      "           2       1.00      0.80      0.89        10\n",
      "           3       0.88      0.88      0.88         8\n",
      "           4       0.88      0.88      0.88         8\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.75      0.67      0.71         9\n",
      "           7       0.92      1.00      0.96        11\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.91      0.90      0.90       100\n",
      "weighted avg       0.91      0.91      0.91       100\n",
      "\n",
      "[[10  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  8  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  7  1  0  0  0  0  0]\n",
      " [ 0  1  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 2  0  0  1  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [88/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55        11\n",
      "           1       1.00      0.83      0.91        12\n",
      "           2       0.75      0.38      0.50         8\n",
      "           3       1.00      0.83      0.91        12\n",
      "           4       0.60      0.86      0.71         7\n",
      "           5       0.71      0.83      0.77         6\n",
      "           6       0.47      0.64      0.55        14\n",
      "           7       1.00      0.86      0.92        14\n",
      "           8       1.00      0.83      0.91         6\n",
      "           9       0.83      1.00      0.91        10\n",
      "\n",
      "    accuracy                           0.76       100\n",
      "   macro avg       0.79      0.76      0.76       100\n",
      "weighted avg       0.79      0.76      0.76       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  5  0  0  0]\n",
      " [ 1 10  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  3  0  3  0  2  0  0  0]\n",
      " [ 0  0  0 10  1  0  1  0  0  0]\n",
      " [ 0  0  0  0  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  1]\n",
      " [ 4  0  1  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 12  0  1]\n",
      " [ 0  0  0  0  0  1  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [89/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.43      0.75      0.55         4\n",
      "           3       0.90      0.90      0.90        10\n",
      "           4       0.91      0.71      0.80        14\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       0.60      0.86      0.71         7\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      0.89      0.94         9\n",
      "           9       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.87      0.87      0.86       100\n",
      "weighted avg       0.92      0.89      0.90       100\n",
      "\n",
      "[[10  0  1  0  0  0  1  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  1  0  0  0]\n",
      " [ 0  0  1  1 10  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  1]\n",
      " [ 0  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [90/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57         7\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.57      0.67      0.62         6\n",
      "           3       1.00      1.00      1.00         9\n",
      "           4       0.91      0.83      0.87        12\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.75      0.75      0.75        12\n",
      "           7       0.92      0.92      0.92        13\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n",
      "[[ 4  0  2  0  0  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  4  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  1  0 10  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 2  0  0  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 14]]\n",
      "\n",
      "\n",
      "<<<test count: [91/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.93      0.88      0.90        16\n",
      "           2       0.80      0.57      0.67         7\n",
      "           3       0.40      0.67      0.50         3\n",
      "           4       0.92      1.00      0.96        12\n",
      "           5       0.89      1.00      0.94         8\n",
      "           6       0.62      0.67      0.64        12\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      0.87      0.93        15\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.84      0.85      0.84       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 14  0  1  0  0  1  0  0  0]\n",
      " [ 1  0  4  0  0  0  2  0  0  0]\n",
      " [ 0  1  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 1  0  1  1  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  1  1  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [92/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86        13\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      0.67      0.80        12\n",
      "           3       1.00      1.00      1.00        14\n",
      "           4       0.60      1.00      0.75         6\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       0.83      0.67      0.74        15\n",
      "           7       0.89      1.00      0.94         8\n",
      "           8       0.86      1.00      0.92         6\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.90      0.91      0.89       100\n",
      "weighted avg       0.91      0.89      0.89       100\n",
      "\n",
      "[[12  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  3  0  1  0  0  0]\n",
      " [ 0  0  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  1  0  0]\n",
      " [ 3  0  0  0  1  0 10  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [93/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61        12\n",
      "           1       1.00      0.75      0.86         4\n",
      "           2       0.83      0.62      0.71         8\n",
      "           3       0.85      0.85      0.85        13\n",
      "           4       0.56      1.00      0.71         5\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.65      0.65      0.65        17\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.85      0.85      0.84       100\n",
      "weighted avg       0.84      0.83      0.83       100\n",
      "\n",
      "[[ 7  0  1  0  1  0  3  0  0  0]\n",
      " [ 0  3  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  2  0  1  0  0  0]\n",
      " [ 0  0  0 11  0  0  2  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 4  0  0  1  1  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [94/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75         9\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.73      0.62      0.67        13\n",
      "           3       1.00      0.71      0.83         7\n",
      "           4       0.71      0.83      0.77        12\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.54      0.78      0.64         9\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.88      0.86      0.87       100\n",
      "weighted avg       0.88      0.86      0.86       100\n",
      "\n",
      "[[ 6  0  2  0  0  0  1  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  2  0  3  0  0  0]\n",
      " [ 0  0  0  5  1  0  1  0  0  0]\n",
      " [ 0  0  1  0 10  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [95/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        11\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       1.00      0.75      0.86         8\n",
      "           3       0.83      1.00      0.91         5\n",
      "           4       0.56      1.00      0.71         5\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.91      0.77      0.83        13\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.92      0.94      0.92       100\n",
      "weighted avg       0.95      0.93      0.93       100\n",
      "\n",
      "[[10  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  6  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  0  0  2  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [96/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71         7\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.88      0.50      0.64        14\n",
      "           3       0.88      0.88      0.88         8\n",
      "           4       0.62      1.00      0.76         8\n",
      "           5       0.80      1.00      0.89         8\n",
      "           6       0.50      0.71      0.59         7\n",
      "           7       1.00      1.00      1.00        12\n",
      "           8       1.00      0.73      0.85        15\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.84      0.85      0.83       100\n",
      "weighted avg       0.87      0.84      0.84       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  7  0  5  0  1  0  0  0]\n",
      " [ 0  0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 1  0  0  1  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  1  0  0  2  1  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [97/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74        10\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.88      0.78      0.82         9\n",
      "           3       0.67      0.86      0.75         7\n",
      "           4       0.77      1.00      0.87        10\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       0.60      0.50      0.55        12\n",
      "           7       1.00      0.80      0.89        10\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.85      0.86      0.85       100\n",
      "weighted avg       0.86      0.85      0.85       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  3  0  0  0]\n",
      " [ 0 13  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  6  0  0  1  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 2  0  1  2  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [98/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85        14\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       1.00      0.67      0.80        12\n",
      "           3       1.00      0.86      0.92         7\n",
      "           4       0.79      0.92      0.85        12\n",
      "           5       1.00      0.92      0.96        13\n",
      "           6       0.50      0.86      0.63         7\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.91      0.90      0.89       100\n",
      "weighted avg       0.92      0.89      0.90       100\n",
      "\n",
      "[[11  0  0  0  0  0  3  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  3  0  1  0  0  0]\n",
      " [ 0  0  0  6  0  0  1  0  0  0]\n",
      " [ 0  0  0  0 11  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  1]\n",
      " [ 1  0  0  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [99/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        10\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       1.00      0.50      0.67         6\n",
      "           3       0.87      1.00      0.93        13\n",
      "           4       1.00      1.00      1.00         9\n",
      "           5       1.00      0.82      0.90        11\n",
      "           6       0.70      1.00      0.82         7\n",
      "           7       0.86      1.00      0.92        12\n",
      "           8       1.00      1.00      1.00        13\n",
      "           9       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.93      0.90      0.90       100\n",
      "weighted avg       0.93      0.92      0.92       100\n",
      "\n",
      "[[ 8  0  0  2  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  3  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  1]\n",
      " [ 0  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [100/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        11\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.71      0.83      0.77         6\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       0.89      1.00      0.94         8\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.89      0.67      0.76        12\n",
      "           7       1.00      0.86      0.92         7\n",
      "           8       0.93      1.00      0.97        14\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.93      0.93      0.92       100\n",
      "weighted avg       0.93      0.93      0.93       100\n",
      "\n",
      "[[10  0  0  0  0  0  0  0  1  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 1  0  2  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_40124\\3316531946.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "model.eval()\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        count += 1\n",
    "        images, labels = images.to(torch.device), labels.to(torch.device)\n",
    "\n",
    "        y_pred = model(images)\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "        y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "\n",
    "        # sklearn.metrics에서는 numpy로 변환하여 작업을 하는데,\n",
    "        # 'cpu'에 있는 tensor만 numpy로 변환 가능함\n",
    "        y_pred_class = y_pred_class.to('cpu')\n",
    "        y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
    "\n",
    "        print(f'<<<test count: [{count}/100]>>>')\n",
    "        print(classification_report(y_test_class, y_pred_class, zero_division=0))\n",
    "        print(confusion_matrix(y_test_class, y_pred_class))\n",
    "        print()\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e9930-c826-4759-bc48-ee7a46f1f5cd",
   "metadata": {},
   "source": [
    "## cf. Neural Network Class를 만드는 또 다른 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bf2be55-ad52-4ff0-a2b8-a721ae08c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new solution\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "model_test_1 = NeuralNetwork().to(torch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab33a56c-d828-4ba1-9014-5fc25e09d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# legacy solution\n",
    "class FashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionDNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=784, out_features=256)\n",
    "        self.layer2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.layer3 = nn.Linear(in_features=128, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "        \n",
    "model_test_2 = FashionDNN().to(torch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "406e64fb-5cbd-4140-af26-e14c308e43c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "FashionDNN(\n",
      "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# compare the ouput between new and legacy solution\n",
    "print(model_test_1)\n",
    "print(model_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3794efaa-5d2d-4646-bd26-38466be8fcf9",
   "metadata": {},
   "source": [
    "(위 cell)  \n",
    "`model`을 호출하는 경우, `__init__`에서 정의된 것들만 리턴한다.  \n",
    "\n",
    "`NeuralNetwork`의 경우, `x`가 Neural Network를 통과하는 과정 순서대로 확인할 수 있다 즉, 가독성이 높다.  \n",
    "`FashionDNN`의 경우, 그렇지 않다.  \n",
    "> 다만 전자가 후자보다 무조건적으로 좋은 것은 아니라고 하는데, 이와 관련한 부분은 지금 레벨에서는 몰라도 되겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f1076-048d-41d4-8987-b432b5f325a4",
   "metadata": {},
   "source": [
    "**cf. `nn.Sequential`**  \n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can user sequential containers to put together a quick network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0be36-590a-4cf9-8710-93bda9a0e48a",
   "metadata": {},
   "source": [
    "# CNN; Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afa0c3-215e-47c6-95ad-b8c7e27aa2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gc_ml_scratch",
   "language": "python",
   "name": "gc_ml_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
