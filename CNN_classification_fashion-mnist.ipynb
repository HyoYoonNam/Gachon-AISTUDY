{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1de03fa-4e5c-4926-a631-a5f6cc742ab8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[PyTorch Tutorials](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#loading-a-dataset)  \n",
    "**Dataset: [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST)**  \n",
    "> The Fashion MNIST dataset is a large freely available database of fashion images that is commonly used for training and testing various machine learning systems.  \n",
    "Fashion-MNIST was intended to serve as a replacement for the original MNIST database for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits.  \n",
    "The dataset contains **70,000 28x28 grayscale images** of fashion products from **10 categories** from a dataset of Zalando article images, with 7,000 images per category.  \n",
    "The **training set consists of 60,000 images** and the **test set consists of 10,000 images**. The dataset is commonly included in standard machine learning libraries.\n",
    "\n",
    "**Framework**  \n",
    "* PyTorch: Version `2.1.0+cuda12.1`\n",
    "\n",
    "**Dependencies**  \n",
    "* Python: Version `3.9.19`\n",
    "* Numpy: Version `1.26.4`\n",
    "* Pandas: Version `2.2.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa5360-9c5e-49b2-aba1-89cd9b9eda58",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450a75a2-aa85-428d-bce6-1d630eb68951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms # for preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b461a91d-f147-43e7-ae2e-7f31c359135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch is running with [cuda:0]\n"
     ]
    }
   ],
   "source": [
    "# gpu가 사용 가능하면 torch가 gpu를 사용하도록 설정, 불가하면 cpu를 사용\n",
    "torch.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'torch is running with [{torch.device}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543bb972-a93c-4b58-abf8-2b23de94b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sample dataset 'FashionMNIST'\n",
    "# 해당 dataset은 애초에 train용, test용이 나눠져 있음\n",
    "\n",
    "# Convert PIL image format to tensor\n",
    "transform = transforms.ToTensor()\n",
    "# download & load the dataset\n",
    "# 초기 다운로드시 100%가 4개이면 모두 완료된 것임\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                  train=True,\n",
    "                                                  download=True,\n",
    "                                                  transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                 train=False,\n",
    "                                                 download=True,\n",
    "                                                 transform=transform)\n",
    "\n",
    "# cf. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#loading-a-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0ef96-8ed8-411b-b032-34f280731c1a",
   "metadata": {},
   "source": [
    "**cf. `ToTensor()`**  \n",
    "[ToTensor](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#totensor) converts a PIL image or Numpy `ndarray` into a `FloatTensor`. and scales the image's pixel intensity values in the range [0., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2cfc29f-b9c3-4d95-8709-31deb8d921cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./dataset\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# download & load 확인\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5308cf-7e8a-4724-9df6-be1f2f2dc75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the dataset\n",
    "\n",
    "# label(0~9)에 따른 제품명 mapping\n",
    "labels_map = {\n",
    "    0: 'T-Shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot'}\n",
    "\n",
    "figure = plt.figure(figsize=(9, 9))\n",
    "cols, rows = 4, 4\n",
    "\n",
    "# train_dataset의 sample을 무작위로 4*4개 만큼 골라서 visualize\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i) # 각 sample을 subplot으로 추가\n",
    "                                      # 1개의 큰 mainplot 안에 4*4개의 subplot\n",
    "    plt.title(labels_map[label]) # 각 subplot에 적용됨\n",
    "    plt.axis('off') # 축 범위 표시 off\n",
    "    # img.shape == (1, 28, 28) == (channel, height, width)임\n",
    "    # .squeeze()는 tensor의 dimension중 size가 1인 dimension을 제거함\n",
    "    # 따라서 img.squeeze() == (28, 28) == (height, width) 즉, 2-dim tensor\n",
    "    # plt.imshow()는 2-dim array를 visualize함\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97396bc4-5f5f-4a11-affa-c01d258760cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "60000\n",
      "28\n",
      "28\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0000, 0.0000, 0.0706, 0.4196, 0.4667, 0.4039,\n",
      "          0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.3882, 0.6078, 0.4431, 0.2392, 0.4627,\n",
      "          0.6784, 0.4588, 0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0039, 0.0000, 0.4314, 0.5333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.6549, 0.6235, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0235,\n",
      "          0.0000, 0.2824, 0.5765, 0.0000, 0.0000, 0.0196, 0.0000, 0.0078,\n",
      "          0.0000, 0.0000, 0.6824, 0.4627, 0.0000, 0.0196, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0196, 0.0000,\n",
      "          0.0000, 0.6824, 0.0157, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.8000, 0.1725, 0.0000, 0.0157, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.0000,\n",
      "          0.4902, 0.5020, 0.0000, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0078, 0.0000, 0.4196, 0.5961, 0.0000, 0.0235, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.7216, 0.0353, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0078, 0.0000, 0.0000, 0.7451, 0.0000, 0.0000, 0.0078,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0078, 0.0000, 0.2863,\n",
      "          0.6196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0157, 0.0000, 0.6157, 0.3098, 0.0000, 0.0118,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0078, 0.0196, 0.0039, 0.0000, 0.0000, 0.6157,\n",
      "          0.3843, 0.0000, 0.0078, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0196, 0.0000, 0.4745, 0.5686, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059,\n",
      "          0.3294, 0.0000, 0.0078, 0.0039, 0.0039, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0039, 0.0000, 0.3608, 0.8745, 0.0627, 0.0000,\n",
      "          0.0275, 0.0157, 0.0039, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.7176,\n",
      "          0.3176, 0.0000, 0.0000, 0.0000, 0.0078, 0.0078, 0.0039, 0.0078,\n",
      "          0.0039, 0.0000, 0.0039, 0.0000, 0.4706, 0.8863, 0.2235, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6784, 0.8471, 0.7569, 0.8353, 0.7176, 0.6431,\n",
      "          0.6549, 0.3843, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.4196, 0.5529, 0.4353, 0.3569,\n",
      "          0.3529, 0.4235, 0.1961, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7255, 0.8667, 0.8510, 0.8235, 0.7922, 0.8706,\n",
      "          0.7843, 0.8078, 0.7922, 0.8000, 0.4275, 0.1059, 0.0471, 0.0667,\n",
      "          0.2392, 0.5333, 0.7059, 0.8667, 0.7922, 0.8824, 0.8157, 0.8392,\n",
      "          0.8745, 0.9412, 0.6353, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7176, 0.8627, 0.8039, 0.7843, 0.7569, 0.7216,\n",
      "          0.7412, 0.7137, 0.6784, 0.7608, 0.8431, 0.8471, 0.8039, 0.8118,\n",
      "          0.8118, 0.7647, 0.7255, 0.7608, 0.7608, 0.7961, 0.8314, 0.7176,\n",
      "          0.7569, 0.8275, 0.6000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7451, 0.9137, 0.7961, 0.8078, 0.8392, 0.8471,\n",
      "          0.7647, 0.7176, 0.6902, 0.6235, 0.6863, 0.7412, 0.7922, 0.7647,\n",
      "          0.7294, 0.7294, 0.7137, 0.7294, 0.7647, 0.8196, 0.9216, 0.7961,\n",
      "          0.7922, 0.8431, 0.5333, 0.0000],\n",
      "         [0.0000, 0.0000, 0.7059, 0.8941, 0.7882, 0.7922, 0.7373, 0.7176,\n",
      "          0.6980, 0.7529, 0.7294, 0.7098, 0.7098, 0.6941, 0.8000, 0.8510,\n",
      "          0.6863, 0.7020, 0.7216, 0.6902, 0.6510, 0.6627, 0.6863, 0.7255,\n",
      "          0.5961, 0.7961, 0.4196, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6549, 0.9216, 0.7843, 0.8118, 0.8118, 0.8196,\n",
      "          0.8118, 0.7961, 0.7765, 0.7412, 0.7137, 0.6980, 0.7294, 0.7412,\n",
      "          0.6980, 0.7529, 0.7725, 0.7647, 0.7529, 0.7255, 0.7216, 0.8039,\n",
      "          0.7333, 1.0000, 0.2392, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5961, 0.9804, 0.8157, 0.8392, 0.8196, 0.7922,\n",
      "          0.7843, 0.7922, 0.8039, 0.8000, 0.7843, 0.7529, 0.7765, 0.8000,\n",
      "          0.7647, 0.8039, 0.8078, 0.7765, 0.7804, 0.7961, 0.8392, 0.8118,\n",
      "          0.7020, 0.9765, 0.2157, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4941, 1.0000, 0.8275, 0.8431, 0.8235, 0.8078,\n",
      "          0.7961, 0.7961, 0.7961, 0.8078, 0.8000, 0.7804, 0.8078, 0.8118,\n",
      "          0.7843, 0.8078, 0.7882, 0.7804, 0.7922, 0.8157, 0.8431, 0.7647,\n",
      "          0.6824, 0.8275, 0.0588, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4196, 1.0000, 0.8314, 0.8275, 0.8235, 0.8275,\n",
      "          0.8157, 0.8078, 0.8078, 0.8118, 0.8157, 0.8157, 0.8510, 0.8392,\n",
      "          0.8039, 0.8078, 0.8000, 0.8157, 0.8275, 0.8275, 0.8627, 0.7725,\n",
      "          0.7137, 0.8824, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2471, 0.9098, 0.8275, 0.8471, 0.8314, 0.8392,\n",
      "          0.8314, 0.8314, 0.8392, 0.8314, 0.8275, 0.8314, 0.8745, 0.8588,\n",
      "          0.8235, 0.8392, 0.8353, 0.8314, 0.8275, 0.8196, 0.8510, 0.8157,\n",
      "          0.6588, 0.7451, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0549, 1.0000, 0.8549, 0.8667, 0.8431, 0.8549,\n",
      "          0.8549, 0.8510, 0.8431, 0.8353, 0.8275, 0.8431, 0.8941, 0.8627,\n",
      "          0.8471, 0.8706, 0.8510, 0.8510, 0.8471, 0.8549, 0.8314, 0.8353,\n",
      "          0.7412, 0.5608, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.9020, 0.8902, 0.8392, 0.8510, 0.8549,\n",
      "          0.8471, 0.8510, 0.8510, 0.8392, 0.8471, 0.8706, 0.8863, 0.8549,\n",
      "          0.8549, 0.8627, 0.8588, 0.8549, 0.8510, 0.8549, 0.8510, 0.8431,\n",
      "          0.8471, 0.4431, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.6941, 0.9137, 0.8471, 0.8745, 0.8706,\n",
      "          0.8353, 0.8353, 0.8431, 0.8314, 0.8627, 0.8863, 0.8667, 0.8627,\n",
      "          0.8667, 0.8549, 0.8706, 0.8627, 0.8627, 0.8667, 0.8510, 0.8314,\n",
      "          0.8588, 0.2039, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0392, 0.8745, 0.8706, 0.8627, 0.8275,\n",
      "          0.8353, 0.8431, 0.8627, 0.8706, 0.8863, 0.8863, 0.8667, 0.8745,\n",
      "          0.8706, 0.8706, 0.8667, 0.8549, 0.8627, 0.8588, 0.8627, 0.8745,\n",
      "          0.6824, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4941, 0.9451, 0.8157, 0.8235,\n",
      "          0.8392, 0.8392, 0.8471, 0.8471, 0.8627, 0.8627, 0.8392, 0.8353,\n",
      "          0.8314, 0.8314, 0.8235, 0.8431, 0.8510, 0.8549, 0.8431, 0.9255,\n",
      "          0.1647, 0.0000, 0.0039, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 0.9294, 0.9020,\n",
      "          0.9137, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000, 1.0000, 1.0000, 0.9059, 0.8980, 0.9373, 0.6314,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2471, 0.3725,\n",
      "          0.4235, 0.4118, 0.3922, 0.4039, 0.4078, 0.4118, 0.4000, 0.3922,\n",
      "          0.3843, 0.3804, 0.3765, 0.3529, 0.3137, 0.3255, 0.2353, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# cf. dataset의 sample 구성\n",
    "# dataset은 2-dimension으로 구성되어 있음\n",
    "# [0]은 샘플의 index, [1]은 해당 샘플의 label\n",
    "print(train_dataset[100][1]) # index 100 sample의 label == target\n",
    "print(len(train_dataset)) # 총 sample 수\n",
    "print(len(train_dataset[100][0][0])) # height pixel. 28\n",
    "print(len(train_dataset[100][0][0][27])) # width pixel. 28\n",
    "print(train_dataset[100][0]) # sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd29eae-d295-461f-94dc-786cc0a3750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 label: 8\n"
     ]
    }
   ],
   "source": [
    "# 기존 label\n",
    "print('기존 label:', train_dataset[100][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5133e304-ab0c-404f-b857-c7dae5994544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label을 one-hot encoding\n",
    "\n",
    "# features에 대한 transform.\n",
    "transform = transforms.ToTensor()\n",
    "# label에 대한 transform. one-hot encoding\n",
    "# PyTorch 연산 편의를 위해 int가 아닌 float로 변환해야됨\n",
    "target_transform = transforms.Lambda(\n",
    "    lambda y: torch.zeros(10, dtype=torch.float32).scatter_(\n",
    "        0, torch.tensor(y), value=1))\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                  train=True,\n",
    "                                                  download=True,\n",
    "                                                  transform=transform,\n",
    "                                                  target_transform=target_transform)\n",
    "\n",
    "# 위에서 이미 train dataset 다운했으므로 추가 다운로드 없이 기존 파일에서 transform진행\n",
    "# cf. https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#totensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b070d0-6c41-4c45-b8f3-9139c3f857d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 과정을 test_dataset에도 적용\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./dataset',\n",
    "                                                  train=False,\n",
    "                                                  download=True,\n",
    "                                                  transform=transform,\n",
    "                                                  target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2ad6b-a1ef-48e5-bc64-1a625eff8abf",
   "metadata": {},
   "source": [
    "**cf. Lambda Transforms**  \n",
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor.  \n",
    "It first creates a zerp tensor of size 10(the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901f8e1-0344-4955-a2e4-76a221b0fd4d",
   "metadata": {},
   "source": [
    "**cf. `transforms.Lambda()`를 사용한 one-hot encoding 자세히 알아보기**  \n",
    "\n",
    "* 기존 label(=y)은 0 ~ 9의 int값을 가졌다는 것을 기억하자.\n",
    "\n",
    "* `import torchvision.transforms as transforms`  \n",
    "  dataset에 대한 여러 preprocessing을 지원한다.\n",
    "  \n",
    "* `transforms.Lambda()`  \n",
    "  user-defined lambda function을 tensor 객체에 적용할 수 있도록 해준다.\n",
    "\n",
    "* `lambda y: `\n",
    "  y(label, 0~9)를 입력받아서 이어지는 명령들을 수행한다.\n",
    "\n",
    "* `torch.zeros(10, dtype=torch.float32)`  \n",
    "  length가 10이고, dtype이 torch.float32인 tensor(array)를 만들어서 zeros로 채운다.\n",
    "  ```python\n",
    "  # cf.\n",
    "  torch.zeros(10, dtype=torch.float32)\n",
    "  > tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "  ```\n",
    "\n",
    "* `.scatter_(0, torch.tensor(y), value=1))`\n",
    "  - `torch.tensor(y)`: 입력받은 y(int)를 tensor로 바꾼다.\n",
    "    ```python\n",
    "    # cf.\n",
    "    y = 8\n",
    "    torch.tensor(y)\n",
    "    > tensor(8)\n",
    "    ```\n",
    "  - `Tensor.scatter(dim, index, src)`  \n",
    "    Tensor 객체(위에서 만든 `tensor([0., 0., ..., 0.])`을 의미)의 `dim`에 대해서 `index`에 해당하는 값을 `src` 값으로 덮어쓴다.  \n",
    "    (cf. PyTorch에서 dim=0이면 row, dim=1이면 column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e840276-8165-467f-9c28-d9780c2c30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoded label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "label mapping: tensor(8)\n"
     ]
    }
   ],
   "source": [
    "# cf. 위의 cf.를 참고할 것.\n",
    "# train_dataset[100][1] 즉, sample index가 100인 sample의 label(y)는 8(int)이었음.\n",
    "# transforms.Lambda()에 의해서 8번 째 index의 0만 1.(float)으로 덮어쓰고, 나머지는 0으로 유지.\n",
    "# 따라서 one-hot encoding됨.\n",
    "print('one-hot encoded label:', train_dataset[100][1])\n",
    "# one-hot encoded vector로부터 본래 label 추출\n",
    "print('label mapping:', torch.argmax(train_dataset[100][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb047038-196b-481c-8e97-93cfe97ff3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset을 dataloader에 batch size만큼씩 전달\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                            batch_size=600)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "#                                           batch_size=100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=600)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3b34cc-afe3-46cf-b11c-f3254dbdfc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# create the model(DenseLayerNeuralNetwork)\n",
    "class FashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionDNN, self).__init__()\n",
    "        # pixel. 28 * 28 = 784\n",
    "        self.layer1 = nn.Linear(in_features=784, out_features=256)\n",
    "        # self.drop = nn.Dropout(0.25)\n",
    "        self.layer2 = nn.Linear(in_features=256, out_features=128)\n",
    "        # label. 0 ~ 9 총 10개\n",
    "        self.layer3 = nn.Linear(in_features=128, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# torch.device가 'cuda'여도 .to(torch.device) 안 해주면\n",
    "# 기본적으로 model은 cpu에 생성됨\n",
    "model = FashionDNN().to(torch.device)\n",
    "print(f'model running on {torch.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e437cb-eaf8-4a6f-94ec-9c9c54c18975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionDNN(\n",
       "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 확인\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4cfa79-ea16-4aa5-9144-d2078adba848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# cf. predict 예제\n",
    "X = torch.rand(1, 28, 28, device=torch.device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted class: {y_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "799cd476-484b-4c5f-b210-f8b408dbbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LossFunction and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b5f0d26-33f5-43dd-9ac8-10dc5f5985d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loader count [10], Loss: 0.1266\n",
      "Epoch [1/20], Loader count [20], Loss: 0.1205\n",
      "Epoch [1/20], Loader count [30], Loss: 0.1374\n",
      "Epoch [1/20], Loader count [40], Loss: 0.1254\n",
      "Epoch [1/20], Loader count [50], Loss: 0.1650\n",
      "Epoch [1/20], Loader count [60], Loss: 0.1235\n",
      "Epoch [1/20], Loader count [70], Loss: 0.1048\n",
      "Epoch [1/20], Loader count [80], Loss: 0.1286\n",
      "Epoch [1/20], Loader count [90], Loss: 0.1178\n",
      "Epoch [1/20], Loader count [100], Loss: 0.1613\n",
      "Epoch [1/20], Loss: 0.1613\n",
      "Epoch [2/20], Loader count [10], Loss: 0.1186\n",
      "Epoch [2/20], Loader count [20], Loss: 0.1358\n",
      "Epoch [2/20], Loader count [30], Loss: 0.1283\n",
      "Epoch [2/20], Loader count [40], Loss: 0.1249\n",
      "Epoch [2/20], Loader count [50], Loss: 0.1517\n",
      "Epoch [2/20], Loader count [60], Loss: 0.1297\n",
      "Epoch [2/20], Loader count [70], Loss: 0.1166\n",
      "Epoch [2/20], Loader count [80], Loss: 0.1406\n",
      "Epoch [2/20], Loader count [90], Loss: 0.1360\n",
      "Epoch [2/20], Loader count [100], Loss: 0.1663\n",
      "Epoch [2/20], Loss: 0.1663\n",
      "Epoch [3/20], Loader count [10], Loss: 0.1071\n",
      "Epoch [3/20], Loader count [20], Loss: 0.1212\n",
      "Epoch [3/20], Loader count [30], Loss: 0.1239\n",
      "Epoch [3/20], Loader count [40], Loss: 0.1147\n",
      "Epoch [3/20], Loader count [50], Loss: 0.1515\n",
      "Epoch [3/20], Loader count [60], Loss: 0.1349\n",
      "Epoch [3/20], Loader count [70], Loss: 0.0865\n",
      "Epoch [3/20], Loader count [80], Loss: 0.1149\n",
      "Epoch [3/20], Loader count [90], Loss: 0.1083\n",
      "Epoch [3/20], Loader count [100], Loss: 0.1726\n",
      "Epoch [3/20], Loss: 0.1726\n",
      "Epoch [4/20], Loader count [10], Loss: 0.1255\n",
      "Epoch [4/20], Loader count [20], Loss: 0.1310\n",
      "Epoch [4/20], Loader count [30], Loss: 0.1244\n",
      "Epoch [4/20], Loader count [40], Loss: 0.1130\n",
      "Epoch [4/20], Loader count [50], Loss: 0.1468\n",
      "Epoch [4/20], Loader count [60], Loss: 0.1089\n",
      "Epoch [4/20], Loader count [70], Loss: 0.1112\n",
      "Epoch [4/20], Loader count [80], Loss: 0.1412\n",
      "Epoch [4/20], Loader count [90], Loss: 0.1264\n",
      "Epoch [4/20], Loader count [100], Loss: 0.1402\n",
      "Epoch [4/20], Loss: 0.1402\n",
      "Epoch [5/20], Loader count [10], Loss: 0.1098\n",
      "Epoch [5/20], Loader count [20], Loss: 0.0981\n",
      "Epoch [5/20], Loader count [30], Loss: 0.1299\n",
      "Epoch [5/20], Loader count [40], Loss: 0.1173\n",
      "Epoch [5/20], Loader count [50], Loss: 0.1449\n",
      "Epoch [5/20], Loader count [60], Loss: 0.1124\n",
      "Epoch [5/20], Loader count [70], Loss: 0.0835\n",
      "Epoch [5/20], Loader count [80], Loss: 0.1186\n",
      "Epoch [5/20], Loader count [90], Loss: 0.1213\n",
      "Epoch [5/20], Loader count [100], Loss: 0.1452\n",
      "Epoch [5/20], Loss: 0.1452\n",
      "Epoch [6/20], Loader count [10], Loss: 0.1066\n",
      "Epoch [6/20], Loader count [20], Loss: 0.1215\n",
      "Epoch [6/20], Loader count [30], Loss: 0.1279\n",
      "Epoch [6/20], Loader count [40], Loss: 0.1220\n",
      "Epoch [6/20], Loader count [50], Loss: 0.1380\n",
      "Epoch [6/20], Loader count [60], Loss: 0.1163\n",
      "Epoch [6/20], Loader count [70], Loss: 0.0839\n",
      "Epoch [6/20], Loader count [80], Loss: 0.1136\n",
      "Epoch [6/20], Loader count [90], Loss: 0.1070\n",
      "Epoch [6/20], Loader count [100], Loss: 0.1286\n",
      "Epoch [6/20], Loss: 0.1286\n",
      "Epoch [7/20], Loader count [10], Loss: 0.1090\n",
      "Epoch [7/20], Loader count [20], Loss: 0.1171\n",
      "Epoch [7/20], Loader count [30], Loss: 0.0958\n",
      "Epoch [7/20], Loader count [40], Loss: 0.1233\n",
      "Epoch [7/20], Loader count [50], Loss: 0.1271\n",
      "Epoch [7/20], Loader count [60], Loss: 0.1031\n",
      "Epoch [7/20], Loader count [70], Loss: 0.1057\n",
      "Epoch [7/20], Loader count [80], Loss: 0.1136\n",
      "Epoch [7/20], Loader count [90], Loss: 0.1189\n",
      "Epoch [7/20], Loader count [100], Loss: 0.1483\n",
      "Epoch [7/20], Loss: 0.1483\n",
      "Epoch [8/20], Loader count [10], Loss: 0.1022\n",
      "Epoch [8/20], Loader count [20], Loss: 0.1073\n",
      "Epoch [8/20], Loader count [30], Loss: 0.1205\n",
      "Epoch [8/20], Loader count [40], Loss: 0.1134\n",
      "Epoch [8/20], Loader count [50], Loss: 0.1267\n",
      "Epoch [8/20], Loader count [60], Loss: 0.1178\n",
      "Epoch [8/20], Loader count [70], Loss: 0.1055\n",
      "Epoch [8/20], Loader count [80], Loss: 0.1426\n",
      "Epoch [8/20], Loader count [90], Loss: 0.1246\n",
      "Epoch [8/20], Loader count [100], Loss: 0.1515\n",
      "Epoch [8/20], Loss: 0.1515\n",
      "Epoch [9/20], Loader count [10], Loss: 0.1002\n",
      "Epoch [9/20], Loader count [20], Loss: 0.1238\n",
      "Epoch [9/20], Loader count [30], Loss: 0.1335\n",
      "Epoch [9/20], Loader count [40], Loss: 0.1278\n",
      "Epoch [9/20], Loader count [50], Loss: 0.1206\n",
      "Epoch [9/20], Loader count [60], Loss: 0.1095\n",
      "Epoch [9/20], Loader count [70], Loss: 0.1059\n",
      "Epoch [9/20], Loader count [80], Loss: 0.1426\n",
      "Epoch [9/20], Loader count [90], Loss: 0.1305\n",
      "Epoch [9/20], Loader count [100], Loss: 0.1491\n",
      "Epoch [9/20], Loss: 0.1491\n",
      "Epoch [10/20], Loader count [10], Loss: 0.1147\n",
      "Epoch [10/20], Loader count [20], Loss: 0.1106\n",
      "Epoch [10/20], Loader count [30], Loss: 0.1223\n",
      "Epoch [10/20], Loader count [40], Loss: 0.1075\n",
      "Epoch [10/20], Loader count [50], Loss: 0.1441\n",
      "Epoch [10/20], Loader count [60], Loss: 0.1278\n",
      "Epoch [10/20], Loader count [70], Loss: 0.0858\n",
      "Epoch [10/20], Loader count [80], Loss: 0.1272\n",
      "Epoch [10/20], Loader count [90], Loss: 0.1207\n",
      "Epoch [10/20], Loader count [100], Loss: 0.1612\n",
      "Epoch [10/20], Loss: 0.1612\n",
      "Epoch [11/20], Loader count [10], Loss: 0.1030\n",
      "Epoch [11/20], Loader count [20], Loss: 0.1283\n",
      "Epoch [11/20], Loader count [30], Loss: 0.1472\n",
      "Epoch [11/20], Loader count [40], Loss: 0.1093\n",
      "Epoch [11/20], Loader count [50], Loss: 0.1333\n",
      "Epoch [11/20], Loader count [60], Loss: 0.1126\n",
      "Epoch [11/20], Loader count [70], Loss: 0.1030\n",
      "Epoch [11/20], Loader count [80], Loss: 0.1372\n",
      "Epoch [11/20], Loader count [90], Loss: 0.1253\n",
      "Epoch [11/20], Loader count [100], Loss: 0.1529\n",
      "Epoch [11/20], Loss: 0.1529\n",
      "Epoch [12/20], Loader count [10], Loss: 0.0837\n",
      "Epoch [12/20], Loader count [20], Loss: 0.0813\n",
      "Epoch [12/20], Loader count [30], Loss: 0.1216\n",
      "Epoch [12/20], Loader count [40], Loss: 0.1269\n",
      "Epoch [12/20], Loader count [50], Loss: 0.1016\n",
      "Epoch [12/20], Loader count [60], Loss: 0.1156\n",
      "Epoch [12/20], Loader count [70], Loss: 0.1003\n",
      "Epoch [12/20], Loader count [80], Loss: 0.1004\n",
      "Epoch [12/20], Loader count [90], Loss: 0.1270\n",
      "Epoch [12/20], Loader count [100], Loss: 0.1404\n",
      "Epoch [12/20], Loss: 0.1404\n",
      "Epoch [13/20], Loader count [10], Loss: 0.0940\n",
      "Epoch [13/20], Loader count [20], Loss: 0.0844\n",
      "Epoch [13/20], Loader count [30], Loss: 0.1163\n",
      "Epoch [13/20], Loader count [40], Loss: 0.1073\n",
      "Epoch [13/20], Loader count [50], Loss: 0.1193\n",
      "Epoch [13/20], Loader count [60], Loss: 0.0968\n",
      "Epoch [13/20], Loader count [70], Loss: 0.1038\n",
      "Epoch [13/20], Loader count [80], Loss: 0.0960\n",
      "Epoch [13/20], Loader count [90], Loss: 0.1078\n",
      "Epoch [13/20], Loader count [100], Loss: 0.1245\n",
      "Epoch [13/20], Loss: 0.1245\n",
      "Epoch [14/20], Loader count [10], Loss: 0.1004\n",
      "Epoch [14/20], Loader count [20], Loss: 0.1064\n",
      "Epoch [14/20], Loader count [30], Loss: 0.1300\n",
      "Epoch [14/20], Loader count [40], Loss: 0.0944\n",
      "Epoch [14/20], Loader count [50], Loss: 0.1174\n",
      "Epoch [14/20], Loader count [60], Loss: 0.1192\n",
      "Epoch [14/20], Loader count [70], Loss: 0.0945\n",
      "Epoch [14/20], Loader count [80], Loss: 0.1171\n",
      "Epoch [14/20], Loader count [90], Loss: 0.1170\n",
      "Epoch [14/20], Loader count [100], Loss: 0.1626\n",
      "Epoch [14/20], Loss: 0.1626\n",
      "Epoch [15/20], Loader count [10], Loss: 0.0824\n",
      "Epoch [15/20], Loader count [20], Loss: 0.1159\n",
      "Epoch [15/20], Loader count [30], Loss: 0.1340\n",
      "Epoch [15/20], Loader count [40], Loss: 0.0867\n",
      "Epoch [15/20], Loader count [50], Loss: 0.1355\n",
      "Epoch [15/20], Loader count [60], Loss: 0.0977\n",
      "Epoch [15/20], Loader count [70], Loss: 0.0855\n",
      "Epoch [15/20], Loader count [80], Loss: 0.1095\n",
      "Epoch [15/20], Loader count [90], Loss: 0.0832\n",
      "Epoch [15/20], Loader count [100], Loss: 0.1490\n",
      "Epoch [15/20], Loss: 0.1490\n",
      "Epoch [16/20], Loader count [10], Loss: 0.0723\n",
      "Epoch [16/20], Loader count [20], Loss: 0.1038\n",
      "Epoch [16/20], Loader count [30], Loss: 0.1013\n",
      "Epoch [16/20], Loader count [40], Loss: 0.0975\n",
      "Epoch [16/20], Loader count [50], Loss: 0.1038\n",
      "Epoch [16/20], Loader count [60], Loss: 0.1023\n",
      "Epoch [16/20], Loader count [70], Loss: 0.0805\n",
      "Epoch [16/20], Loader count [80], Loss: 0.0961\n",
      "Epoch [16/20], Loader count [90], Loss: 0.0808\n",
      "Epoch [16/20], Loader count [100], Loss: 0.1425\n",
      "Epoch [16/20], Loss: 0.1425\n",
      "Epoch [17/20], Loader count [10], Loss: 0.0754\n",
      "Epoch [17/20], Loader count [20], Loss: 0.1130\n",
      "Epoch [17/20], Loader count [30], Loss: 0.0878\n",
      "Epoch [17/20], Loader count [40], Loss: 0.1052\n",
      "Epoch [17/20], Loader count [50], Loss: 0.1171\n",
      "Epoch [17/20], Loader count [60], Loss: 0.0956\n",
      "Epoch [17/20], Loader count [70], Loss: 0.1165\n",
      "Epoch [17/20], Loader count [80], Loss: 0.1143\n",
      "Epoch [17/20], Loader count [90], Loss: 0.0859\n",
      "Epoch [17/20], Loader count [100], Loss: 0.1252\n",
      "Epoch [17/20], Loss: 0.1252\n",
      "Epoch [18/20], Loader count [10], Loss: 0.0888\n",
      "Epoch [18/20], Loader count [20], Loss: 0.0970\n",
      "Epoch [18/20], Loader count [30], Loss: 0.1098\n",
      "Epoch [18/20], Loader count [40], Loss: 0.0856\n",
      "Epoch [18/20], Loader count [50], Loss: 0.1103\n",
      "Epoch [18/20], Loader count [60], Loss: 0.1097\n",
      "Epoch [18/20], Loader count [70], Loss: 0.0835\n",
      "Epoch [18/20], Loader count [80], Loss: 0.1007\n",
      "Epoch [18/20], Loader count [90], Loss: 0.1144\n",
      "Epoch [18/20], Loader count [100], Loss: 0.1221\n",
      "Epoch [18/20], Loss: 0.1221\n",
      "Epoch [19/20], Loader count [10], Loss: 0.0865\n",
      "Epoch [19/20], Loader count [20], Loss: 0.0844\n",
      "Epoch [19/20], Loader count [30], Loss: 0.1042\n",
      "Epoch [19/20], Loader count [40], Loss: 0.0774\n",
      "Epoch [19/20], Loader count [50], Loss: 0.1194\n",
      "Epoch [19/20], Loader count [60], Loss: 0.1672\n",
      "Epoch [19/20], Loader count [70], Loss: 0.0888\n",
      "Epoch [19/20], Loader count [80], Loss: 0.1019\n",
      "Epoch [19/20], Loader count [90], Loss: 0.1124\n",
      "Epoch [19/20], Loader count [100], Loss: 0.1334\n",
      "Epoch [19/20], Loss: 0.1334\n",
      "Epoch [20/20], Loader count [10], Loss: 0.0854\n",
      "Epoch [20/20], Loader count [20], Loss: 0.0794\n",
      "Epoch [20/20], Loader count [30], Loss: 0.1154\n",
      "Epoch [20/20], Loader count [40], Loss: 0.0888\n",
      "Epoch [20/20], Loader count [50], Loss: 0.1729\n",
      "Epoch [20/20], Loader count [60], Loss: 0.1363\n",
      "Epoch [20/20], Loader count [70], Loss: 0.1115\n",
      "Epoch [20/20], Loader count [80], Loss: 0.0860\n",
      "Epoch [20/20], Loader count [90], Loss: 0.1366\n",
      "Epoch [20/20], Loader count [100], Loss: 0.1200\n",
      "Epoch [20/20], Loss: 0.1200\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    count = 0\n",
    "    # images, labels 둘 다 tensor 객체\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(torch.device), labels.to(torch.device)\n",
    "\n",
    "        # train = Variable(images.view(100, 1, 28, 28))\n",
    "        # labels = Variable(labels)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if not (count % 10):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loader count [{count}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfa7f1eb-ed5d-4195-af16-11e71c10b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [1/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.73      0.79      0.76        14\n",
      "           3       1.00      0.78      0.88         9\n",
      "           4       0.88      0.70      0.78        10\n",
      "           5       0.90      1.00      0.95         9\n",
      "           6       0.67      0.75      0.71         8\n",
      "           7       0.91      0.91      0.91        11\n",
      "           8       0.92      1.00      0.96        12\n",
      "           9       1.00      0.67      0.80         6\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.88      0.86      0.86       100\n",
      "weighted avg       0.88      0.87      0.87       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 11  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  7  0  0  2  0  0  0]\n",
      " [ 0  0  3  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  1  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  4]]\n",
      "\n",
      "\n",
      "<<<test count: [2/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.81      1.00      0.90        13\n",
      "           3       0.67      1.00      0.80         8\n",
      "           4       1.00      0.55      0.71        11\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.78      0.88      0.82         8\n",
      "           7       1.00      0.89      0.94         9\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.92      0.91      0.90       100\n",
      "weighted avg       0.92      0.90      0.90       100\n",
      "\n",
      "[[10  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 13  0  1  0  0  0  0  0  0]\n",
      " [ 0  0 13  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  3  2  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [3/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.71      0.83      0.77        12\n",
      "           3       0.75      0.86      0.80         7\n",
      "           4       0.75      0.67      0.71         9\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.90      0.75      0.82        12\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[12  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  6  1  0  0  0  0  0]\n",
      " [ 0  0  2  1  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 1  0  2  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [4/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.60      0.63        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.78      0.47      0.58        15\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.79      0.85      0.81        13\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.47      0.82      0.60        11\n",
      "           7       0.92      1.00      0.96        11\n",
      "           8       1.00      0.60      0.75         5\n",
      "           9       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.86      0.82      0.83       100\n",
      "weighted avg       0.84      0.81      0.81       100\n",
      "\n",
      "[[ 6  0  0  0  1  0  3  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 2  0  7  0  2  0  4  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  1  0  0  0  1  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [5/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        13\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.80      0.73      0.76        11\n",
      "           3       0.90      0.90      0.90        10\n",
      "           4       0.87      0.93      0.90        14\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.75      0.75      0.75         8\n",
      "           7       0.83      0.71      0.77         7\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.88      0.88      0.88       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n",
      "[[11  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  8  0  1  0  1  0  0  0]\n",
      " [ 1  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  1  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  1  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  2]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [6/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78         7\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.90      0.82      0.86        11\n",
      "           3       0.88      0.78      0.82         9\n",
      "           4       0.73      0.80      0.76        10\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.90      0.75      0.82        12\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.91      0.90       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  2  0  0  0  0  0]\n",
      " [ 1  0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  1  1  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 2  0  0  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 1  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [7/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85        14\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.85      0.92      0.88        12\n",
      "           3       1.00      0.89      0.94         9\n",
      "           4       0.87      0.87      0.87        15\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.64      0.90      0.75        10\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      0.82      0.90        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.93      0.92      0.92       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[11  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  2  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 1  0  0  0  1  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [8/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        11\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.67      0.50      0.57         8\n",
      "           3       0.70      1.00      0.82         7\n",
      "           4       0.57      0.89      0.70         9\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       1.00      0.45      0.62        11\n",
      "           7       0.91      0.91      0.91        11\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.87      0.85      0.84       100\n",
      "weighted avg       0.89      0.86      0.85       100\n",
      "\n",
      "[[11  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  1  0  0  0  0  0  0]\n",
      " [ 1  0  4  1  2  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  1  0  0]\n",
      " [ 0  0  1  1  4  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [9/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91         6\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.75      0.75      0.75         8\n",
      "           3       0.92      1.00      0.96        11\n",
      "           4       0.70      0.70      0.70        10\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.75      0.75      0.75         4\n",
      "           7       1.00      0.93      0.96        14\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.92      0.92      0.92       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  2  0  0  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  0  0]\n",
      " [ 0  0  2  1  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [10/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85        14\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.86      0.86      0.86         7\n",
      "           3       0.83      0.91      0.87        11\n",
      "           4       0.83      0.71      0.77        14\n",
      "           5       1.00      0.80      0.89        10\n",
      "           6       0.75      0.92      0.83        13\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.89      0.88      0.88       100\n",
      "\n",
      "[[11  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  1  0  0  0  0  0]\n",
      " [ 1  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  1  1 10  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  1  0  1]\n",
      " [ 0  0  0  0  1  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [11/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77         7\n",
      "           1       0.90      0.82      0.86        11\n",
      "           2       0.80      0.80      0.80         5\n",
      "           3       0.92      0.92      0.92        13\n",
      "           4       0.67      0.91      0.77        11\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       0.78      0.64      0.70        11\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       0.89      0.89      0.89         9\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.88      0.87      0.87       100\n",
      "weighted avg       0.89      0.88      0.88       100\n",
      "\n",
      "[[ 5  0  0  0  1  0  1  0  0  0]\n",
      " [ 0  9  0  1  1  0  0  0  0  0]\n",
      " [ 0  0  4  0  1  0  0  0  0  0]\n",
      " [ 0  1  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  1  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0]\n",
      " [ 1  0  0  0  2  0  7  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [12/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.75      0.75      0.75        12\n",
      "           3       0.67      1.00      0.80         4\n",
      "           4       0.78      0.93      0.85        15\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.77      0.62      0.69        16\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.86      0.87      0.86       100\n",
      "weighted avg       0.86      0.86      0.86       100\n",
      "\n",
      "[[ 3  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  2  0  1  0  0  0]\n",
      " [ 0  0  0  4  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 14  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  1]\n",
      " [ 1  0  3  0  2  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [13/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80        16\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.93      0.87      0.90        15\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       0.86      0.86      0.86        14\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.50      0.62      0.56         8\n",
      "           7       1.00      1.00      1.00         4\n",
      "           8       0.88      1.00      0.93         7\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.90      0.91      0.90       100\n",
      "weighted avg       0.90      0.89      0.89       100\n",
      "\n",
      "[[12  0  0  0  0  0  3  0  1  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 13  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  1  0 12  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 2  0  0  0  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [14/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71         8\n",
      "           1       0.90      1.00      0.95         9\n",
      "           2       0.78      0.58      0.67        12\n",
      "           3       0.82      0.75      0.78        12\n",
      "           4       0.72      0.87      0.79        15\n",
      "           5       0.88      1.00      0.93         7\n",
      "           6       0.73      0.89      0.80         9\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.86      0.84      0.84       100\n",
      "weighted avg       0.85      0.84      0.84       100\n",
      "\n",
      "[[ 5  0  0  1  1  0  1  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  1  3  0  1  0  0  0]\n",
      " [ 1  1  0  9  0  0  1  0  0  0]\n",
      " [ 0  0  2  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  4]]\n",
      "\n",
      "\n",
      "<<<test count: [15/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.89      0.80      0.84        10\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       0.70      0.78      0.74         9\n",
      "           5       1.00      1.00      1.00        15\n",
      "           6       0.55      0.67      0.60         9\n",
      "           7       0.92      1.00      0.96        12\n",
      "           8       1.00      0.83      0.91         6\n",
      "           9       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.90      0.89      0.89       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 1  0  1  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [16/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88         8\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       1.00      0.58      0.74        12\n",
      "           3       1.00      1.00      1.00        11\n",
      "           4       0.71      1.00      0.83         5\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.64      0.88      0.74         8\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.92      0.93      0.92       100\n",
      "weighted avg       0.95      0.93      0.93       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  7  0  1  0  3  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [17/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60         6\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.75      0.43      0.55         7\n",
      "           3       0.93      0.87      0.90        15\n",
      "           4       0.69      0.92      0.79        12\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.55      0.75      0.63         8\n",
      "           7       0.87      1.00      0.93        13\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.85      0.81      0.82       100\n",
      "weighted avg       0.87      0.85      0.85       100\n",
      "\n",
      "[[ 3  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  9  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  2  0  2  0  0  0]\n",
      " [ 0  0  0 13  2  0  0  0  0  0]\n",
      " [ 0  0  1  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [18/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70         9\n",
      "           1       1.00      0.75      0.86         4\n",
      "           2       1.00      0.91      0.95        11\n",
      "           3       1.00      0.80      0.89         5\n",
      "           4       0.89      1.00      0.94         8\n",
      "           5       0.91      0.91      0.91        11\n",
      "           6       0.89      0.89      0.89        19\n",
      "           7       0.92      0.92      0.92        13\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       0.92      0.92      0.92        13\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.92      0.89      0.90       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  2  0  0  0]\n",
      " [ 1  3  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  1  0  0  0  0  0]\n",
      " [ 1  0  0  4  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  1  0  0]\n",
      " [ 2  0  0  0  0  0 17  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  1  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [19/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        13\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.78      1.00      0.88         7\n",
      "           3       0.92      0.85      0.88        13\n",
      "           4       0.89      0.67      0.76        12\n",
      "           5       0.89      1.00      0.94         8\n",
      "           6       0.33      1.00      0.50         1\n",
      "           7       1.00      0.91      0.95        11\n",
      "           8       0.92      1.00      0.96        11\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.87      0.93      0.88       100\n",
      "weighted avg       0.94      0.92      0.92       100\n",
      "\n",
      "[[12  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 11  1  0  0  0  1  0]\n",
      " [ 0  0  2  1  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [20/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.89      0.67      0.76        12\n",
      "           3       0.80      0.89      0.84         9\n",
      "           4       0.33      0.67      0.44         3\n",
      "           5       1.00      0.88      0.93         8\n",
      "           6       0.64      0.64      0.64        11\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.84      0.85      0.83       100\n",
      "weighted avg       0.88      0.86      0.87       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 14  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  2  0  2  0  0  0]\n",
      " [ 0  0  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  1  0  0]\n",
      " [ 2  0  0  0  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [21/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         6\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      0.50      0.67        12\n",
      "           3       0.91      0.77      0.83        13\n",
      "           4       0.53      0.89      0.67         9\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       0.78      0.78      0.78         9\n",
      "           7       0.92      0.92      0.92        13\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.88      0.87      0.86       100\n",
      "weighted avg       0.89      0.86      0.86       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  1  4  0  1  0  0  0]\n",
      " [ 1  0  0 10  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  1  0  0]\n",
      " [ 1  0  0  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [22/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.91      1.00      0.95        10\n",
      "           3       1.00      0.87      0.93        15\n",
      "           4       0.75      0.90      0.82        10\n",
      "           5       0.91      1.00      0.95        10\n",
      "           6       0.71      0.56      0.62         9\n",
      "           7       1.00      0.93      0.97        15\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.91      0.93      0.92       100\n",
      "weighted avg       0.92      0.92      0.92       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  1  0  1  0  0  0]\n",
      " [ 0  0  0  0  9  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  1  0  2  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 14  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [23/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74        14\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.86      0.55      0.67        11\n",
      "           3       1.00      0.89      0.94         9\n",
      "           4       0.82      0.90      0.86        10\n",
      "           5       0.93      0.93      0.93        15\n",
      "           6       0.22      0.40      0.29         5\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.85      0.84      0.84       100\n",
      "weighted avg       0.88      0.85      0.86       100\n",
      "\n",
      "[[10  0  0  0  0  1  3  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  2  0  3  0  0  0]\n",
      " [ 0  0  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  1]\n",
      " [ 3  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [24/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        12\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.75      0.60      0.67         5\n",
      "           3       0.91      0.77      0.83        13\n",
      "           4       0.69      1.00      0.82         9\n",
      "           5       0.91      1.00      0.95        10\n",
      "           6       0.88      0.78      0.82         9\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      0.94      0.97        16\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.89      0.89       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[11  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  3  0  2  0  0  0  0  0]\n",
      " [ 1  0  0 10  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  0  1  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 15  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [25/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        10\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.67      0.80      0.73         5\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       0.71      0.71      0.71        14\n",
      "           5       0.90      0.90      0.90        10\n",
      "           6       0.71      0.42      0.53        12\n",
      "           7       0.67      1.00      0.80         2\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.82      0.86      0.83       100\n",
      "weighted avg       0.85      0.84      0.84       100\n",
      "\n",
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  1  0  0  0  0  0  0]\n",
      " [ 1  0  4  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 11  1  0  0  0  0  0]\n",
      " [ 0  0  2  0 10  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 4  0  0  0  3  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [26/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.73      0.67      0.70        12\n",
      "           3       0.60      0.75      0.67         4\n",
      "           4       0.60      0.75      0.67         8\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.33      0.25      0.29         8\n",
      "           7       0.92      0.86      0.89        14\n",
      "           8       0.89      1.00      0.94         8\n",
      "           9       0.87      0.93      0.90        14\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.78      0.80      0.78       100\n",
      "weighted avg       0.81      0.81      0.81       100\n",
      "\n",
      "[[ 9  0  1  0  0  0  1  0  1  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  3  1  0  0  0  0  0]\n",
      " [ 0  0  1  1  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 2  0  1  1  2  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [27/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        11\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.75      0.67      0.71         9\n",
      "           3       0.80      0.73      0.76        11\n",
      "           4       0.82      0.82      0.82        17\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       0.56      0.75      0.64        12\n",
      "           7       1.00      1.00      1.00        12\n",
      "           8       1.00      0.90      0.95        10\n",
      "           9       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.87      0.86      0.86       100\n",
      "weighted avg       0.85      0.84      0.84       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  3  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  2  0  1  0  0  0]\n",
      " [ 1  0  0  8  1  0  1  0  0  0]\n",
      " [ 1  0  0  1 14  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  0]\n",
      " [ 0  0  2  1  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]]\n",
      "\n",
      "\n",
      "<<<test count: [28/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86         7\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       1.00      0.83      0.91         6\n",
      "           3       0.83      0.91      0.87        11\n",
      "           4       0.93      0.93      0.93        15\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       0.80      1.00      0.89         8\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       1.00      1.00      1.00        15\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.94       100\n",
      "   macro avg       0.94      0.94      0.94       100\n",
      "weighted avg       0.95      0.94      0.94       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  1  0  0  0]\n",
      " [ 1 11  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  1  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  1  0  0  0]\n",
      " [ 0  0  0  1 14  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [29/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71        13\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.91      0.91      0.91        11\n",
      "           3       1.00      0.71      0.83        14\n",
      "           4       0.36      1.00      0.53         4\n",
      "           5       1.00      0.89      0.94         9\n",
      "           6       0.78      0.47      0.58        15\n",
      "           7       0.89      1.00      0.94         8\n",
      "           8       0.89      1.00      0.94         8\n",
      "           9       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.85      0.87      0.84       100\n",
      "weighted avg       0.87      0.83      0.83       100\n",
      "\n",
      "[[10  0  0  0  0  0  2  0  1  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  1  0  0  0  0  0]\n",
      " [ 1  0  0 10  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  1  0  0]\n",
      " [ 4  0  1  0  3  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [30/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.91      0.67      0.77        15\n",
      "           3       0.75      0.50      0.60         6\n",
      "           4       0.57      0.89      0.70         9\n",
      "           5       0.86      1.00      0.92         6\n",
      "           6       0.65      0.79      0.71        14\n",
      "           7       1.00      0.67      0.80         9\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.80      1.00      0.89         8\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.84      0.82      0.82       100\n",
      "weighted avg       0.85      0.82      0.82       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 11  0  1  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  2  0  3  0  0  0]\n",
      " [ 1  0  0  3  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  0]\n",
      " [ 0  0  1  0  2  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  6  0  2]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [31/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       1.00      0.92      0.96        12\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       0.75      0.86      0.80         7\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.67      1.00      0.80         4\n",
      "           7       0.93      0.93      0.93        14\n",
      "           8       0.94      0.94      0.94        16\n",
      "           9       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.87      0.88      0.87       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 3  0  0  0  0  0  2  0  1  0]\n",
      " [ 0  9  0  0  1  0  0  0  0  0]\n",
      " [ 0  0 11  0  1  0  0  0  0  0]\n",
      " [ 1  0  0 11  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  1]\n",
      " [ 1  0  0  0  0  0  0  0 15  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [32/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        10\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.73      1.00      0.84         8\n",
      "           3       1.00      0.89      0.94         9\n",
      "           4       0.80      0.89      0.84         9\n",
      "           5       0.86      1.00      0.92         6\n",
      "           6       0.92      0.80      0.86        15\n",
      "           7       1.00      1.00      1.00        16\n",
      "           8       1.00      0.92      0.96        13\n",
      "           9       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.93      0.94      0.93       100\n",
      "weighted avg       0.94      0.93      0.93       100\n",
      "\n",
      "[[ 9  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  8  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  0]\n",
      " [ 0  0  2  0  1  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 16  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [33/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80        13\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.83      0.67      0.74        15\n",
      "           3       0.75      0.86      0.80         7\n",
      "           4       0.69      0.82      0.75        11\n",
      "           5       1.00      0.92      0.96        13\n",
      "           6       0.56      0.71      0.62         7\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.85      0.86      0.86       100\n",
      "weighted avg       0.86      0.85      0.85       100\n",
      "\n",
      "[[10  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 10  0  2  0  2  0  0  0]\n",
      " [ 0  0  1  6  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  1  0]\n",
      " [ 1  0  0  0  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [34/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        14\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.67      0.67      0.67         9\n",
      "           3       0.86      0.75      0.80         8\n",
      "           4       0.44      0.80      0.57         5\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.67      0.44      0.53         9\n",
      "           7       1.00      0.91      0.95        11\n",
      "           8       0.94      1.00      0.97        16\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.83      0.84      0.83       100\n",
      "weighted avg       0.86      0.85      0.85       100\n",
      "\n",
      "[[11  0  1  0  0  0  1  0  1  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  3  0  0  0  0  0]\n",
      " [ 1  0  1  6  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  4  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 1  0  1  1  2  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 16  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [35/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56         9\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.90      0.64      0.75        14\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       0.75      0.90      0.82        10\n",
      "           5       0.92      0.92      0.92        13\n",
      "           6       0.56      0.50      0.53        10\n",
      "           7       1.00      0.92      0.96        12\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.82      0.83      0.82       100\n",
      "weighted avg       0.82      0.82      0.82       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  3  0  1  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  2  3  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  1]\n",
      " [ 4  0  1  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  1]\n",
      " [ 0  0  0  0  0  1  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [36/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80        13\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.70      0.54      0.61        13\n",
      "           3       0.86      0.86      0.86         7\n",
      "           4       0.50      0.60      0.55         5\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.38      0.60      0.46         5\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.83      0.84      0.83       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[10  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  7  0  2  0  3  0  0  0]\n",
      " [ 1  0  0  6  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 0  0  1  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [37/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.88      0.74         8\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       1.00      0.86      0.92         7\n",
      "           3       0.90      0.90      0.90        10\n",
      "           4       0.85      0.92      0.88        12\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       1.00      0.67      0.80         9\n",
      "           7       0.90      0.90      0.90        10\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.87      0.93      0.90        14\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.91      0.90      0.90       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 7  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  6  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 2  0  0  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [38/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78        11\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.58      0.78      0.67         9\n",
      "           3       0.89      0.73      0.80        11\n",
      "           4       0.67      0.67      0.67        12\n",
      "           5       0.67      0.67      0.67         3\n",
      "           6       0.82      0.75      0.78        12\n",
      "           7       1.00      0.90      0.95        10\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.83      0.82      0.82       100\n",
      "weighted avg       0.84      0.83      0.83       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  0  0  0  0]\n",
      " [ 2  0  0  8  1  0  0  0  0  0]\n",
      " [ 0  0  3  1  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  1]\n",
      " [ 1  0  1  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  9  0  0]\n",
      " [ 0  0  1  0  0  0  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [39/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.43      0.50      0.46         6\n",
      "           3       0.67      0.89      0.76         9\n",
      "           4       0.70      0.70      0.70        10\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.71      0.45      0.56        11\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.83      0.83      0.83       100\n",
      "weighted avg       0.85      0.85      0.85       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  1  2  0  0  0  0  0]\n",
      " [ 1  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  1  2  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0]\n",
      " [ 1  0  3  1  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [40/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.78      0.70      0.74        10\n",
      "           3       0.85      0.85      0.85        13\n",
      "           4       0.62      0.89      0.73         9\n",
      "           5       0.90      1.00      0.95         9\n",
      "           6       0.67      0.75      0.71         8\n",
      "           7       0.88      0.88      0.88         8\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.86      0.84      0.84       100\n",
      "weighted avg       0.86      0.85      0.85       100\n",
      "\n",
      "[[ 7  0  1  1  0  0  1  0  0  0]\n",
      " [ 0 15  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  1  0  0  0]\n",
      " [ 1  0  0 11  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  7  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [41/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.77      0.83        13\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.86      0.55      0.67        11\n",
      "           3       0.86      0.86      0.86         7\n",
      "           4       0.67      0.83      0.74        12\n",
      "           5       0.90      0.90      0.90        10\n",
      "           6       0.58      0.88      0.70         8\n",
      "           7       0.90      0.90      0.90        10\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.87      0.86      0.86       100\n",
      "weighted avg       0.87      0.85      0.85       100\n",
      "\n",
      "[[10  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  1  3  0  1  0  0  0]\n",
      " [ 0  0  0  6  1  0  0  0  0  0]\n",
      " [ 0  0  1  0 10  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 1  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  9  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [42/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.83      0.74        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.86      0.71      0.77        17\n",
      "           3       0.83      0.91      0.87        11\n",
      "           4       0.56      0.83      0.67         6\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.88      0.50      0.64        14\n",
      "           7       1.00      0.75      0.86         4\n",
      "           8       0.82      1.00      0.90         9\n",
      "           9       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.85      0.85      0.84       100\n",
      "weighted avg       0.85      0.83      0.83       100\n",
      "\n",
      "[[10  0  1  1  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 12  1  2  0  1  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  1  0]\n",
      " [ 0  0  1  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0]\n",
      " [ 4  0  0  0  2  0  7  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  3  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [43/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        12\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.82      0.95      0.88        19\n",
      "           3       1.00      0.83      0.91         6\n",
      "           4       0.60      0.50      0.55         6\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.82      0.69      0.75        13\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00        13\n",
      "           9       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.91      0.89      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[11  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 18  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  5  1  0  0  0  0  0]\n",
      " [ 0  0  2  0  3  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 2  0  2  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [44/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83         7\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.78      0.78      0.78         9\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       0.85      0.85      0.85        13\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.71      0.83      0.77         6\n",
      "           7       1.00      1.00      1.00        12\n",
      "           8       0.92      1.00      0.96        12\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.92      0.91      0.91       100\n",
      "weighted avg       0.92      0.92      0.92       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  0  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  1  0]\n",
      " [ 0  0  1  1 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [45/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86         7\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.60      0.75      0.67         4\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       1.00      0.86      0.92        14\n",
      "           5       0.80      0.89      0.84         9\n",
      "           6       0.70      0.78      0.74         9\n",
      "           7       0.80      0.67      0.73         6\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  1  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  1  1 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  1  0  0]\n",
      " [ 1  0  1  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [46/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        10\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      0.88      0.93         8\n",
      "           3       0.67      1.00      0.80         6\n",
      "           4       0.86      0.86      0.86        14\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       0.78      0.64      0.70        11\n",
      "           7       0.83      1.00      0.91        10\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       0.90      0.82      0.86        11\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.89      0.89      0.88       100\n",
      "weighted avg       0.90      0.89      0.89       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 14  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  6  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  1]\n",
      " [ 0  0  0  2  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [47/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        10\n",
      "           1       1.00      0.88      0.93         8\n",
      "           2       1.00      0.62      0.76        13\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       0.64      0.90      0.75        10\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.70      0.88      0.78         8\n",
      "           7       1.00      0.88      0.93         8\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.90      0.89      0.88       100\n",
      "weighted avg       0.90      0.88      0.88       100\n",
      "\n",
      "[[ 8  0  0  1  0  0  1  0  0  0]\n",
      " [ 1  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  4  0  1  0  0  0]\n",
      " [ 0  0  0 11  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [48/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.50      0.67      0.57         6\n",
      "           3       0.88      0.93      0.90        15\n",
      "           4       0.85      0.73      0.79        15\n",
      "           5       1.00      0.87      0.93        15\n",
      "           6       0.75      0.75      0.75         8\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       0.89      0.89      0.89         9\n",
      "           9       0.71      1.00      0.83         5\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.85      0.87      0.85       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  0  0  1  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  1  0  1  0  0  0]\n",
      " [ 0  0  0 14  1  0  0  0  0  0]\n",
      " [ 0  0  3  1 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  2]\n",
      " [ 1  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [49/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71         6\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.78      0.47      0.58        15\n",
      "           3       0.86      0.86      0.86         7\n",
      "           4       0.50      0.71      0.59         7\n",
      "           5       0.93      1.00      0.96        13\n",
      "           6       0.58      0.64      0.61        11\n",
      "           7       0.89      0.89      0.89         9\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.81      0.82      0.81       100\n",
      "weighted avg       0.82      0.81      0.81       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  4  0  4  0  0  0]\n",
      " [ 0  0  0  6  0  0  1  0  0  0]\n",
      " [ 0  0  2  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 3  0  0  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  1]\n",
      " [ 0  0  0  0  0  0  0  1  0 14]]\n",
      "\n",
      "\n",
      "<<<test count: [50/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.62      0.73        13\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.56      0.83      0.67         6\n",
      "           3       0.88      0.88      0.88        16\n",
      "           4       0.75      0.60      0.67        10\n",
      "           5       1.00      0.88      0.93         8\n",
      "           6       0.44      0.67      0.53         6\n",
      "           7       1.00      0.86      0.92         7\n",
      "           8       1.00      1.00      1.00        15\n",
      "           9       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.84      0.83      0.82       100\n",
      "weighted avg       0.86      0.84      0.84       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  5  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  1  0  0  0  0  0]\n",
      " [ 1  0  1 14  0  0  0  0  0  0]\n",
      " [ 0  0  2  2  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  1]\n",
      " [ 0  0  1  0  1  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [51/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.80      0.80      0.80        10\n",
      "           3       0.78      1.00      0.88        14\n",
      "           4       0.50      0.56      0.53         9\n",
      "           5       1.00      0.94      0.97        16\n",
      "           6       0.88      0.58      0.70        12\n",
      "           7       0.80      1.00      0.89         8\n",
      "           8       0.67      0.67      0.67         3\n",
      "           9       0.91      0.83      0.87        12\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.83      0.83      0.82       100\n",
      "weighted avg       0.85      0.84      0.84       100\n",
      "\n",
      "[[10  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  2  0  0  0  0  0]\n",
      " [ 0  0  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  2  2  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  1]\n",
      " [ 0  0  0  1  3  0  7  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  2  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [52/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.62         9\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.89      1.00      0.94         8\n",
      "           3       1.00      0.86      0.92         7\n",
      "           4       0.86      1.00      0.92        18\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.69      0.69      0.69        13\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.92      0.90      0.91       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  6  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 1  0  1  0  2  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [53/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88        13\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.71      0.71      0.71         7\n",
      "           3       0.81      0.81      0.81        16\n",
      "           4       0.82      0.90      0.86        10\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.70      0.70      0.70        10\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.89      0.89      0.89       100\n",
      "\n",
      "[[11  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  1  0  1  0  0  0]\n",
      " [ 1  0  0 13  1  0  1  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  1  2  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n",
      "\n",
      "\n",
      "<<<test count: [54/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        10\n",
      "           1       1.00      0.86      0.92         7\n",
      "           2       0.60      1.00      0.75         3\n",
      "           3       0.60      0.60      0.60         5\n",
      "           4       0.80      0.80      0.80        15\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.67      0.80      0.73         5\n",
      "           7       1.00      0.93      0.97        15\n",
      "           8       1.00      1.00      1.00        17\n",
      "           9       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.86      0.88      0.86       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 8  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  6  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  3  2  0  0  0  0  0]\n",
      " [ 0  0  2  0 12  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [55/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.85      0.69      0.76        16\n",
      "           3       0.71      0.83      0.77         6\n",
      "           4       0.69      0.90      0.78        10\n",
      "           5       1.00      0.93      0.96        14\n",
      "           6       0.60      0.60      0.60        10\n",
      "           7       1.00      0.89      0.94         9\n",
      "           8       1.00      0.67      0.80         6\n",
      "           9       0.89      1.00      0.94        16\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.84      0.83      0.83       100\n",
      "weighted avg       0.85      0.84      0.84       100\n",
      "\n",
      "[[ 4  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  3  0  2  0  0  0]\n",
      " [ 0  0  0  5  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  1]\n",
      " [ 2  0  1  1  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  1]\n",
      " [ 0  0  1  1  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 16]]\n",
      "\n",
      "\n",
      "<<<test count: [56/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        10\n",
      "           1       1.00      0.87      0.93        15\n",
      "           2       0.69      0.90      0.78        10\n",
      "           3       0.80      0.67      0.73        12\n",
      "           4       0.42      0.71      0.53         7\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.78      0.54      0.64        13\n",
      "           7       0.91      0.91      0.91        11\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.84      0.83      0.83       100\n",
      "weighted avg       0.84      0.82      0.82       100\n",
      "\n",
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  1  1  0  0  0  0  0]\n",
      " [ 0  0  9  0  1  0  0  0  0  0]\n",
      " [ 0  0  1  8  3  0  0  0  0  0]\n",
      " [ 0  0  0  1  5  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 1  0  3  0  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [57/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.92      1.00      0.96        11\n",
      "           3       1.00      0.95      0.97        19\n",
      "           4       0.67      0.86      0.75         7\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.80      0.67      0.73        12\n",
      "           7       0.86      1.00      0.92         6\n",
      "           8       0.90      1.00      0.95         9\n",
      "           9       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.89      0.91      0.90       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  2  0  0  0]\n",
      " [ 1  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 18  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 1  0  1  0  2  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [58/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75         8\n",
      "           1       1.00      0.80      0.89        10\n",
      "           2       1.00      0.67      0.80        12\n",
      "           3       0.75      1.00      0.86         9\n",
      "           4       1.00      0.91      0.95        11\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.64      1.00      0.78         7\n",
      "           7       1.00      1.00      1.00        10\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.91      0.90      0.89       100\n",
      "weighted avg       0.92      0.90      0.90       100\n",
      "\n",
      "[[ 6  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  8  0  1  0  0  1  0  0  0]\n",
      " [ 2  0  8  1  0  0  1  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [59/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      0.82      0.90        11\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       0.77      0.91      0.83        11\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.83      0.77      0.80        13\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.94      0.94      0.94       100\n",
      "weighted avg       0.94      0.93      0.93       100\n",
      "\n",
      "[[ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 1  0  0  0  2  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [60/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       1.00      0.60      0.75        10\n",
      "           3       0.87      0.93      0.90        14\n",
      "           4       0.50      1.00      0.67         6\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.71      0.50      0.59        10\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.86      0.86      0.84       100\n",
      "weighted avg       0.87      0.85      0.85       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  6  0  3  0  0  0  0  0]\n",
      " [ 0  0  0 13  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 2  0  0  1  2  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [61/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78        11\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.85      0.92      0.88        12\n",
      "           3       0.67      0.80      0.73         5\n",
      "           4       0.83      0.83      0.83         6\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.71      0.50      0.59        10\n",
      "           7       1.00      0.86      0.92         7\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.89      0.89      0.89       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  4  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 3  0  2  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n",
      "\n",
      "\n",
      "<<<test count: [62/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.78      0.88      0.82         8\n",
      "           3       1.00      0.90      0.95        10\n",
      "           4       0.69      0.90      0.78        10\n",
      "           5       1.00      0.93      0.96        14\n",
      "           6       1.00      0.67      0.80         9\n",
      "           7       0.88      1.00      0.93         7\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.93      0.93      0.92       100\n",
      "weighted avg       0.94      0.93      0.93       100\n",
      "\n",
      "[[ 4  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  9  1  0  0  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  1  0  0]\n",
      " [ 0  0  1  0  2  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [63/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86         7\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.79      0.83      0.81        18\n",
      "           3       0.93      0.93      0.93        15\n",
      "           4       0.69      0.75      0.72        12\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.82      0.75      0.78        12\n",
      "           7       1.00      0.80      0.89         5\n",
      "           8       1.00      0.83      0.91         6\n",
      "           9       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.90      0.88      0.88       100\n",
      "weighted avg       0.87      0.87      0.87       100\n",
      "\n",
      "[[ 6  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 15  0  2  0  1  0  0  0]\n",
      " [ 0  0  0 14  1  0  0  0  0  0]\n",
      " [ 0  0  3  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 1  0  0  1  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0  1]\n",
      " [ 0  0  0  0  0  0  1  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [64/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83         6\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.90      1.00      0.95         9\n",
      "           3       1.00      1.00      1.00        13\n",
      "           4       1.00      1.00      1.00         6\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       1.00      0.92      0.96        12\n",
      "           7       1.00      0.90      0.95        10\n",
      "           8       0.90      1.00      0.95         9\n",
      "           9       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.95      0.95      0.95       100\n",
      "weighted avg       0.96      0.95      0.95       100\n",
      "\n",
      "[[ 5  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 1  0  0  0  0  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [65/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       0.80      0.89      0.84         9\n",
      "           3       0.73      0.80      0.76        10\n",
      "           4       0.90      0.69      0.78        13\n",
      "           5       0.91      1.00      0.95        10\n",
      "           6       0.60      0.75      0.67         8\n",
      "           7       1.00      0.91      0.95        11\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.88      0.87      0.87       100\n",
      "\n",
      "[[ 7  0  0  0  1  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  1  0  0  0  0  0  0]\n",
      " [ 0  1  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  1  1  9  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 10  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  7]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [66/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.82      0.75      0.78        12\n",
      "           3       1.00      0.86      0.92         7\n",
      "           4       0.53      0.89      0.67         9\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.80      0.63      0.71        19\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       0.92      0.92      0.92        13\n",
      "           9       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.88      0.88      0.88       100\n",
      "weighted avg       0.87      0.85      0.85       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  6  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 1  0  2  0  3  0 12  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  1 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [67/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53        10\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      0.67      0.80         9\n",
      "           3       0.88      0.70      0.78        10\n",
      "           4       0.45      1.00      0.62         5\n",
      "           5       1.00      0.82      0.90        11\n",
      "           6       0.58      0.58      0.58        12\n",
      "           7       0.83      1.00      0.91        10\n",
      "           8       1.00      1.00      1.00        16\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.83      0.83      0.81       100\n",
      "weighted avg       0.85      0.82      0.82       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  4  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  3  0  0  0  0  0]\n",
      " [ 1  0  0  7  1  0  1  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  2  0  0]\n",
      " [ 3  0  0  0  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 16  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [68/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        13\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      0.86      0.92         7\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.91      1.00      0.95        10\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.75      0.60      0.67         5\n",
      "           7       0.85      0.92      0.88        12\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.93      0.92      0.92       100\n",
      "weighted avg       0.93      0.93      0.93       100\n",
      "\n",
      "[[12  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  1  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  1  0  0]\n",
      " [ 2  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 16]]\n",
      "\n",
      "\n",
      "<<<test count: [69/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.86      0.50      0.63        12\n",
      "           3       0.80      1.00      0.89        12\n",
      "           4       0.86      0.75      0.80         8\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.64      0.78      0.70         9\n",
      "           7       1.00      1.00      1.00        13\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.91      0.90      0.90       100\n",
      "weighted avg       0.91      0.90      0.89       100\n",
      "\n",
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  6  1  1  0  3  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  1  1  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [70/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62         8\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      0.67      0.80        12\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       0.73      1.00      0.84         8\n",
      "           5       1.00      1.00      1.00        15\n",
      "           6       0.64      0.78      0.70         9\n",
      "           7       0.92      1.00      0.96        11\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.91      0.89      0.89       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  3  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  3  0  1  0  0  0]\n",
      " [ 1  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 2  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [71/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94         8\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       0.85      0.69      0.76        16\n",
      "           3       0.89      0.89      0.89         9\n",
      "           4       0.33      0.67      0.44         6\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       1.00      0.64      0.78        11\n",
      "           7       1.00      1.00      1.00         9\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.90      0.89      0.88       100\n",
      "weighted avg       0.92      0.88      0.89       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  8  1  0  0  0  0  0]\n",
      " [ 0  0  1  1  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 1  0  1  0  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [72/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.73        14\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       0.67      0.75      0.71         8\n",
      "           3       0.83      0.83      0.83        12\n",
      "           4       0.75      0.86      0.80         7\n",
      "           5       0.89      0.89      0.89         9\n",
      "           6       0.82      0.60      0.69        15\n",
      "           7       0.83      0.83      0.83         6\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       0.92      0.92      0.92        13\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.84      0.85      0.84       100\n",
      "weighted avg       0.83      0.83      0.83       100\n",
      "\n",
      "[[11  0  1  0  0  0  2  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  1  1  0  0  0  0  0]\n",
      " [ 2  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  1]\n",
      " [ 3  0  1  1  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [73/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.75      0.82      0.78        11\n",
      "           3       0.90      1.00      0.95         9\n",
      "           4       0.73      0.80      0.76        10\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.60      0.43      0.50         7\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.89      0.89      0.89       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[10  0  1  1  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  2  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0]\n",
      " [ 1  0  2  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [74/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.79      0.79      0.79        14\n",
      "           3       1.00      0.82      0.90        17\n",
      "           4       0.76      0.87      0.81        15\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.75      0.75      0.75         8\n",
      "           7       1.00      1.00      1.00         4\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.89      0.90      0.89       100\n",
      "weighted avg       0.89      0.88      0.88       100\n",
      "\n",
      "[[ 5  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 11  0  1  0  1  0  0  0]\n",
      " [ 1  0  0 14  2  0  0  0  0  0]\n",
      " [ 0  0  2  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  1]\n",
      " [ 0  0  1  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [75/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      0.78      0.88         9\n",
      "           3       1.00      1.00      1.00         9\n",
      "           4       0.91      1.00      0.95        10\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.92      0.92      0.92        12\n",
      "           7       1.00      0.86      0.92         7\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.96       100\n",
      "   macro avg       0.96      0.96      0.96       100\n",
      "weighted avg       0.96      0.96      0.96       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 1  0  0  0  0  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [76/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       1.00      0.80      0.89        10\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.89      0.89      0.89        18\n",
      "           4       0.94      0.94      0.94        16\n",
      "           5       0.86      0.75      0.80         8\n",
      "           6       0.75      0.86      0.80         7\n",
      "           7       0.88      1.00      0.93         7\n",
      "           8       1.00      0.89      0.94         9\n",
      "           9       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.91      0.91      0.91       100\n",
      "weighted avg       0.91      0.91      0.91       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  1  0  0  1  0  0  0]\n",
      " [ 0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  1  0  1  0  0  0]\n",
      " [ 0  0  0  1 15  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  1  0  1]\n",
      " [ 1  0  0  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [77/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.90      0.69      0.78        13\n",
      "           3       0.91      0.77      0.83        13\n",
      "           4       0.50      0.80      0.62         5\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.75      0.86      0.80         7\n",
      "           7       1.00      0.92      0.96        12\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.89      0.90      0.89       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[12  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  9  1  2  0  0  0  0  0]\n",
      " [ 0  0  0 10  1  0  2  0  0  0]\n",
      " [ 0  0  1  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [78/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78         9\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.86      0.67      0.75         9\n",
      "           3       1.00      0.90      0.95        10\n",
      "           4       0.86      0.75      0.80         8\n",
      "           5       1.00      0.88      0.93        16\n",
      "           6       0.71      0.62      0.67         8\n",
      "           7       0.94      0.89      0.91        18\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       0.73      1.00      0.84         8\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.87      0.87      0.86       100\n",
      "weighted avg       0.89      0.87      0.87       100\n",
      "\n",
      "[[ 9  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  6  0  1  0  1  0  0  0]\n",
      " [ 1  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  1  0  1]\n",
      " [ 3  0  0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 16  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [79/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67        13\n",
      "           1       1.00      0.86      0.92         7\n",
      "           2       0.75      0.86      0.80         7\n",
      "           3       1.00      0.85      0.92        13\n",
      "           4       0.70      1.00      0.82         7\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.67      0.73      0.70        11\n",
      "           7       0.93      1.00      0.97        14\n",
      "           8       0.86      0.86      0.86         7\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.86      0.87      0.86       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  4  0  1  0]\n",
      " [ 0  6  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  6  0  1  0  0  0  0  0]\n",
      " [ 1  0  1 11  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  1  0  0]\n",
      " [ 1  0  1  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [80/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       1.00      0.75      0.86         8\n",
      "           3       0.70      0.88      0.78         8\n",
      "           4       0.80      0.86      0.83        14\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       1.00      0.67      0.80        12\n",
      "           7       0.91      0.91      0.91        11\n",
      "           8       0.91      1.00      0.95        10\n",
      "           9       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.89      0.87      0.87       100\n",
      "weighted avg       0.89      0.87      0.87       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 12  0  1  0  0  0  0  0  0]\n",
      " [ 1  0  6  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  7  1  0  0  0  0  0]\n",
      " [ 0  0  0  1 12  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 3  0  0  0  1  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [81/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.86        18\n",
      "           1       1.00      0.83      0.91         6\n",
      "           2       0.86      0.55      0.67        11\n",
      "           3       0.70      0.78      0.74         9\n",
      "           4       0.45      0.71      0.56         7\n",
      "           5       1.00      0.92      0.96        12\n",
      "           6       0.50      0.56      0.53         9\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.93      1.00      0.97        14\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.83      0.82      0.82       100\n",
      "weighted avg       0.84      0.82      0.82       100\n",
      "\n",
      "[[15  0  0  1  0  0  2  0  0  0]\n",
      " [ 0  5  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  3  0  2  0  0  0]\n",
      " [ 1  0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  1  1  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  1]\n",
      " [ 1  0  0  0  3  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]]\n",
      "\n",
      "\n",
      "<<<test count: [82/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.67      0.67      0.67         6\n",
      "           3       0.78      0.78      0.78         9\n",
      "           4       0.80      0.92      0.86        13\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       0.89      0.80      0.84        10\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.88      0.88      0.88       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  4  1  1  0  0  0  0  0]\n",
      " [ 1  0  0  7  1  0  0  0  0  0]\n",
      " [ 0  0  1  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [83/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88         7\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.83      0.83      0.83        12\n",
      "           3       1.00      0.78      0.88         9\n",
      "           4       0.60      0.75      0.67         8\n",
      "           5       0.93      0.88      0.90        16\n",
      "           6       0.79      0.73      0.76        15\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.86      0.86      0.85       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  1  0  1  0  0  0]\n",
      " [ 1  0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  1  0  1]\n",
      " [ 1  0  1  0  2  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  3]]\n",
      "\n",
      "\n",
      "<<<test count: [84/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        15\n",
      "           1       1.00      0.93      0.97        15\n",
      "           2       0.67      1.00      0.80         4\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.89      0.89      0.89         9\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.75      0.86      0.80         7\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.94       100\n",
      "   macro avg       0.92      0.95      0.93       100\n",
      "weighted avg       0.95      0.94      0.94       100\n",
      "\n",
      "[[12  0  0  0  1  0  2  0  0  0]\n",
      " [ 0 14  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [85/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       0.88      0.88      0.88         8\n",
      "           3       0.86      0.86      0.86         7\n",
      "           4       0.92      0.79      0.85        14\n",
      "           5       1.00      0.89      0.94         9\n",
      "           6       0.56      0.83      0.67         6\n",
      "           7       0.92      0.92      0.92        12\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.90      0.91      0.90       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[11  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  6  1  0  0  0  0  0]\n",
      " [ 0  0  1  0 11  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  1  0  0]\n",
      " [ 0  0  0  1  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [86/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88         7\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.89      0.73      0.80        11\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.91      0.91      0.91        11\n",
      "           5       0.86      0.86      0.86         7\n",
      "           6       0.89      0.80      0.84        10\n",
      "           7       0.92      0.92      0.92        13\n",
      "           8       1.00      1.00      1.00        12\n",
      "           9       0.89      0.89      0.89         9\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.90      0.91      0.90       100\n",
      "weighted avg       0.91      0.91      0.91       100\n",
      "\n",
      "[[ 7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  8  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  1  0  0]\n",
      " [ 1  0  1  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [87/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83        11\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.88      0.70      0.78        10\n",
      "           3       0.78      0.88      0.82         8\n",
      "           4       0.67      0.75      0.71         8\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.71      0.56      0.62         9\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00        11\n",
      "           9       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.88      0.88      0.88       100\n",
      "weighted avg       0.89      0.89      0.89       100\n",
      "\n",
      "[[10  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  7  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  7  1  0  0  0  0  0]\n",
      " [ 0  0  1  1  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 2  0  0  1  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<test count: [88/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        11\n",
      "           1       1.00      0.83      0.91        12\n",
      "           2       0.50      0.50      0.50         8\n",
      "           3       0.91      0.83      0.87        12\n",
      "           4       0.60      0.86      0.71         7\n",
      "           5       0.86      1.00      0.92         6\n",
      "           6       0.67      0.71      0.69        14\n",
      "           7       1.00      0.93      0.96        14\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.83      0.84      0.83       100\n",
      "weighted avg       0.84      0.83      0.83       100\n",
      "\n",
      "[[ 8  0  1  0  0  0  2  0  0  0]\n",
      " [ 1 10  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  4  0  2  0  2  0  0  0]\n",
      " [ 0  0  1 10  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  0]\n",
      " [ 1  0  1  1  1  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [89/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.50      0.50      0.50         4\n",
      "           3       0.91      1.00      0.95        10\n",
      "           4       0.92      0.79      0.85        14\n",
      "           5       0.71      1.00      0.83         5\n",
      "           6       0.60      0.86      0.71         7\n",
      "           7       1.00      0.94      0.97        17\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.86      0.88      0.87       100\n",
      "weighted avg       0.92      0.90      0.90       100\n",
      "\n",
      "[[10  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  1  0  1  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  1  1 11  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 16  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  9]]\n",
      "\n",
      "\n",
      "<<<test count: [90/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71         7\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.67      0.67      0.67         6\n",
      "           3       0.90      1.00      0.95         9\n",
      "           4       0.91      0.83      0.87        12\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.75      0.75      0.75        12\n",
      "           7       0.92      0.85      0.88        13\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.88      0.93      0.90        15\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n",
      "[[ 5  0  1  0  0  0  1  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  1  1 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 2  0  0  0  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  2]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 14]]\n",
      "\n",
      "\n",
      "<<<test count: [91/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        11\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       0.62      0.71      0.67         7\n",
      "           3       0.50      0.67      0.57         3\n",
      "           4       0.85      0.92      0.88        12\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       0.70      0.58      0.64        12\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      0.93      0.97        15\n",
      "           9       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.84      0.86      0.85       100\n",
      "weighted avg       0.87      0.87      0.87       100\n",
      "\n",
      "[[ 9  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 15  0  1  0  0  0  0  0  0]\n",
      " [ 1  0  5  0  0  0  1  0  0  0]\n",
      " [ 0  1  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  1  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 1  0  2  0  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [92/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71        13\n",
      "           1       0.88      1.00      0.93         7\n",
      "           2       1.00      0.92      0.96        12\n",
      "           3       0.93      0.93      0.93        14\n",
      "           4       0.75      1.00      0.86         6\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.82      0.60      0.69        15\n",
      "           7       1.00      0.88      0.93         8\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.90      0.91      0.90       100\n",
      "weighted avg       0.89      0.88      0.88       100\n",
      "\n",
      "[[10  1  0  0  0  0  2  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  1  0  0  0  0  0]\n",
      " [ 1  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0]\n",
      " [ 4  0  0  1  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n",
      "<<<test count: [93/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.71      0.62      0.67         8\n",
      "           3       0.85      0.85      0.85        13\n",
      "           4       0.50      1.00      0.67         5\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.69      0.53      0.60        17\n",
      "           7       0.89      1.00      0.94         8\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.84      0.86      0.84       100\n",
      "weighted avg       0.84      0.83      0.83       100\n",
      "\n",
      "[[ 9  0  0  0  1  0  2  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  1  0  2  0  0  0]\n",
      " [ 0  0  0 11  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 3  0  2  2  1  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  7]]\n",
      "\n",
      "\n",
      "<<<test count: [94/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.90      0.69      0.78        13\n",
      "           3       1.00      0.71      0.83         7\n",
      "           4       0.58      0.92      0.71        12\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.88      0.78      0.82         9\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.94      0.90      0.91       100\n",
      "weighted avg       0.93      0.90      0.90       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  5  2  0  0  0  0  0]\n",
      " [ 0  0  1  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 0  0  0  0  2  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]]\n",
      "\n",
      "\n",
      "<<<test count: [95/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87        11\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       1.00      0.75      0.86         8\n",
      "           3       0.71      1.00      0.83         5\n",
      "           4       0.71      1.00      0.83         5\n",
      "           5       1.00      0.90      0.95        10\n",
      "           6       0.90      0.69      0.78        13\n",
      "           7       0.93      1.00      0.96        13\n",
      "           8       0.92      1.00      0.96        11\n",
      "           9       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.90      0.92      0.90       100\n",
      "weighted avg       0.92      0.91      0.91       100\n",
      "\n",
      "[[10  0  0  0  0  0  1  0  0  0]\n",
      " [ 1 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 1  0  0  2  0  0  9  0  1  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "<<<test count: [96/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77         7\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.90      0.64      0.75        14\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.67      1.00      0.80         8\n",
      "           5       0.89      1.00      0.94         8\n",
      "           6       0.62      0.71      0.67         7\n",
      "           7       0.92      0.92      0.92        12\n",
      "           8       1.00      0.87      0.93        15\n",
      "           9       0.89      0.89      0.89         9\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.86      0.87      0.86       100\n",
      "weighted avg       0.88      0.87      0.87       100\n",
      "\n",
      "[[ 5  0  0  1  0  0  1  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  3  0  2  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  1]\n",
      " [ 0  0  1  0  0  1  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  8]]\n",
      "\n",
      "\n",
      "<<<test count: [97/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       1.00      0.86      0.92        14\n",
      "           2       0.78      0.78      0.78         9\n",
      "           3       0.70      1.00      0.82         7\n",
      "           4       0.69      0.90      0.78        10\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       0.67      0.50      0.57        12\n",
      "           7       1.00      0.90      0.95        10\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.86      0.87      0.86       100\n",
      "weighted avg       0.87      0.86      0.86       100\n",
      "\n",
      "[[ 8  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 12  0  1  1  0  0  0  0  0]\n",
      " [ 0  0  7  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 1  0  2  2  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11]]\n",
      "\n",
      "\n",
      "<<<test count: [98/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        14\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.91      0.83      0.87        12\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       0.77      0.83      0.80        12\n",
      "           5       1.00      1.00      1.00        13\n",
      "           6       0.71      0.71      0.71         7\n",
      "           7       0.89      1.00      0.94         8\n",
      "           8       1.00      1.00      1.00         9\n",
      "           9       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.92      0.91      0.92       100\n",
      "weighted avg       0.92      0.92      0.92       100\n",
      "\n",
      "[[13  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  1  0 10  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  5]]\n",
      "\n",
      "\n",
      "<<<test count: [99/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78        10\n",
      "           1       0.92      0.92      0.92        12\n",
      "           2       1.00      0.67      0.80         6\n",
      "           3       0.93      1.00      0.96        13\n",
      "           4       0.90      1.00      0.95         9\n",
      "           5       0.92      1.00      0.96        11\n",
      "           6       0.67      0.86      0.75         7\n",
      "           7       0.92      0.92      0.92        12\n",
      "           8       0.92      0.92      0.92        13\n",
      "           9       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.88      0.89       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "[[ 7  1  0  1  0  0  1  0  0  0]\n",
      " [ 0 11  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  2  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  6  0  1  0]\n",
      " [ 0  0  0  0  0  1  0 11  0  0]\n",
      " [ 1  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  6]]\n",
      "\n",
      "\n",
      "<<<test count: [100/100]>>>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87        11\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       0.71      0.83      0.77         6\n",
      "           3       0.86      0.86      0.86         7\n",
      "           4       0.89      1.00      0.94         8\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.78      0.58      0.67        12\n",
      "           7       1.00      0.86      0.92         7\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       0.86      1.00      0.92        12\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.89      0.89      0.89       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[10  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  6  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  1]\n",
      " [ 2  0  2  0  1  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
      "C:\\Users\\komando\\AppData\\Local\\Temp\\ipykernel_37588\\36659087.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "model.eval()\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        count += 1\n",
    "        images, labels = images.to(torch.device), labels.to(torch.device)\n",
    "\n",
    "        y_pred = model(images)\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "        y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "\n",
    "        # sklearn.metrics에서는 numpy로 변환하여 작업을 하는데,\n",
    "        # 'cpu'에 있는 tensor만 numpy로 변환 가능함\n",
    "        y_pred_class = y_pred_class.to('cpu')\n",
    "        y_test_class = torch.argmax(torch.tensor(labels, dtype=torch.int64), dim=1).to('cpu')\n",
    "\n",
    "        print(f'<<<test count: [{count}/100]>>>')\n",
    "        print(classification_report(y_test_class, y_pred_class, zero_division=0))\n",
    "        print(confusion_matrix(y_test_class, y_pred_class))\n",
    "        print()\n",
    "        print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gc_ml_scratch",
   "language": "python",
   "name": "gc_ml_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
